"""
SAC æ™ºèƒ½ä½“ - CTDE æ¶æ„ç‰ˆæœ¬

å…³é”®åŒºåˆ«ï¼š
- Actorï¼šåˆ†æ•£å¼ï¼Œåªç”¨æœ¬åœ°è§‚æµ‹
- Criticï¼šé›†ä¸­å¼ï¼Œç”¨å…¨å±€çŠ¶æ€ + è”åˆåŠ¨ä½œ
"""
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

from config import (
    DEVICE, STATE_DIM, HIDDEN_DIM, ACTION_DIM, NUM_AGENTS,
    LEARNING_RATE, ALPHA_LR, GAMMA, TAU, BATCH_SIZE,
    INIT_ALPHA, GRADIENT_STEPS, NUM_FOLLOWERS, GLOBAL_STATE_DIM
)
from buffer import CTDEReplayBuffer
from networks import GaussianActor, SoftQNetwork


class CTDESACAgent:
    """
    CTDE SAC æ™ºèƒ½ä½“
    
    Centralized Training:
    - Critic ä½¿ç”¨å…¨å±€çŠ¶æ€ + æ‰€æœ‰æ™ºèƒ½ä½“çš„è”åˆåŠ¨ä½œ
    
    Decentralized Execution:
    - Actor åªä½¿ç”¨å•ä¸ªæ™ºèƒ½ä½“çš„æœ¬åœ°è§‚æµ‹
    """
    
    def __init__(self, topology, auto_entropy=True, use_amp=True):
        self.topology = topology
        self.num_followers = topology.num_followers
        self.num_agents = topology.num_agents
        self.auto_entropy = auto_entropy
        self.use_amp = use_amp and torch.cuda.is_available()
        
        # åˆ†æ•£å¼ Actorï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“å…±äº«å‚æ•°ï¼‰
        self.actor = GaussianActor(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        
        # é›†ä¸­å¼ Criticï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰
        self.q1 = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2 = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q1_target = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2_target = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        
        self.q1_target.load_state_dict(self.q1.state_dict())
        self.q2_target.load_state_dict(self.q2.state_dict())
        
        for param in self.q1_target.parameters():
            param.requires_grad = False
        for param in self.q2_target.parameters():
            param.requires_grad = False
        
        if self.use_amp:
            from torch.cuda.amp import GradScaler
            self.scaler = GradScaler()
            print("ğŸš€ AMP (æ··åˆç²¾åº¦è®­ç»ƒ) å·²å¯ç”¨ - CTDE æ¶æ„")
        else:
            self.scaler = None
        
        # æ¸©åº¦å‚æ•°ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“å…±äº«ï¼‰
        # å¯¹äºå¤šæ™ºèƒ½ä½“è”åˆåŠ¨ä½œç©ºé—´ï¼Œtarget_entropy åº”è€ƒè™‘æ‰€æœ‰æ™ºèƒ½ä½“
        self.target_entropy = -float(ACTION_DIM * self.num_followers)
        self.log_alpha = torch.tensor(np.log(INIT_ALPHA), requires_grad=True, device=DEVICE)
        self.alpha = self.log_alpha.exp().item()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE)
        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=LEARNING_RATE)
        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=LEARNING_RATE)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=ALPHA_LR)
        
        # CTDE ç¼“å†²åŒº
        self.buffer = CTDEReplayBuffer(num_agents=NUM_AGENTS)
        
        self.last_losses = {'q1': 0, 'q2': 0, 'actor': 0, 'alpha': INIT_ALPHA}
        self.update_count = 0
        
        print(f"ğŸ“Š CTDE Agent initialized:")
        print(f"   Actor input: Local state ({STATE_DIM})")
        print(f"   Critic input: Global state ({GLOBAL_STATE_DIM}) + Joint action ({NUM_FOLLOWERS * ACTION_DIM})")
    
    @torch.no_grad()
    def select_action(self, local_states, deterministic=False):
        """
        åˆ†æ•£å¼åŠ¨ä½œé€‰æ‹©ï¼ˆåªç”¨æœ¬åœ°è§‚æµ‹ï¼‰
        
        Args:
            local_states: (batch, num_agents, state_dim) æˆ– (num_agents, state_dim)
        """
        is_batched = local_states.dim() == 3
        
        if is_batched:
            batch_size = local_states.shape[0]
            # åªå¤„ç†è·Ÿéšè€…
            follower_states = local_states[:, 1:, :]
            flat_states = follower_states.reshape(-1, STATE_DIM)
            
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    action, _, _ = self.actor(flat_states, deterministic=deterministic)
            else:
                action, _, _ = self.actor(flat_states, deterministic=deterministic)
            
            action = action.view(batch_size, self.num_followers, ACTION_DIM)
        else:
            follower_states = local_states[1:, :]
            
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    action, _, _ = self.actor(follower_states, deterministic=deterministic)
            else:
                action, _, _ = self.actor(follower_states, deterministic=deterministic)
        
        return action.float()
    
    def store_transitions_batch(self, local_states, global_states, actions, rewards, 
                                next_local_states, next_global_states, dones):
        """æ‰¹é‡å­˜å‚¨ï¼ˆåŒ…å«å…¨å±€çŠ¶æ€ï¼‰"""
        self.buffer.push_batch(local_states, global_states, actions, rewards, 
                               next_local_states, next_global_states, dones)
    
    def update(self, batch_size=BATCH_SIZE, gradient_steps=GRADIENT_STEPS):
        """æ›´æ–°ç½‘ç»œï¼ˆCTDE æ–¹å¼ï¼‰"""
        if not self.buffer.is_ready(batch_size):
            return {}
        
        total_q1_loss = 0
        total_q2_loss = 0
        total_actor_loss = 0
        
        for _ in range(gradient_steps):
            self.update_count += 1
            
            # é‡‡æ ·
            (local_states, global_states, actions, rewards, 
             next_local_states, next_global_states, dones) = self.buffer.sample(batch_size)
            
            # å‡†å¤‡æ•°æ®
            follower_states = local_states[:, 1:, :].reshape(-1, STATE_DIM)
            follower_next_states = next_local_states[:, 1:, :].reshape(-1, STATE_DIM)
            joint_actions = actions.view(batch_size, -1)  # (batch, num_followers * action_dim)
            
            # ========== Critic æ›´æ–°ï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰==========
            with torch.no_grad():
                # ä½¿ç”¨ Actor ç”Ÿæˆä¸‹ä¸€æ­¥åŠ¨ä½œ
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        next_actions, next_log_probs, _ = self.actor(follower_next_states)
                else:
                    next_actions, next_log_probs, _ = self.actor(follower_next_states)
                
                # é‡å¡‘ä¸ºè”åˆåŠ¨ä½œ
                next_joint_actions = next_actions.view(batch_size, -1)

                # ğŸ”§ å¤šæ™ºèƒ½ä½“ç†µé¡¹ï¼šåº”å¯¹è”åˆç­–ç•¥çš„ log-prob åšâ€œæ±‚å’Œâ€è€Œä¸æ˜¯å‡å€¼
                # next_log_probs: (batch*num_followers, 1) -> (batch, num_followers, 1) -> (batch, 1)
                next_log_probs_joint = next_log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                
                # ä½¿ç”¨å…¨å±€çŠ¶æ€è®¡ç®— Q å€¼
                q1_next = self.q1_target(next_global_states, next_joint_actions)
                q2_next = self.q2_target(next_global_states, next_joint_actions)
                q_next = torch.min(q1_next, q2_next)
                
                target_q = rewards.unsqueeze(1) + GAMMA * (1 - dones.unsqueeze(1)) * (q_next - self.alpha * next_log_probs_joint)
                target_q = target_q.float()
            
            # Q1 æ›´æ–°
            self.q1_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    q1_curr = self.q1(global_states, joint_actions)
                    q1_loss = F.mse_loss(q1_curr.float(), target_q)
                self.scaler.scale(q1_loss).backward()
                self.scaler.unscale_(self.q1_optimizer)
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                self.scaler.step(self.q1_optimizer)
            else:
                q1_curr = self.q1(global_states, joint_actions)
                q1_loss = F.mse_loss(q1_curr, target_q)
                q1_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                self.q1_optimizer.step()
            
            # Q2 æ›´æ–°
            self.q2_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    q2_curr = self.q2(global_states, joint_actions)
                    q2_loss = F.mse_loss(q2_curr.float(), target_q)
                self.scaler.scale(q2_loss).backward()
                self.scaler.unscale_(self.q2_optimizer)
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)
                self.scaler.step(self.q2_optimizer)
            else:
                q2_curr = self.q2(global_states, joint_actions)
                q2_loss = F.mse_loss(q2_curr, target_q)
                q2_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)
                self.q2_optimizer.step()
            
            # ========== Actor æ›´æ–° ==========
            self.actor_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    new_actions, log_probs, _ = self.actor(follower_states)
                    new_joint_actions = new_actions.view(batch_size, -1)
                    
                    # ä½¿ç”¨å…¨å±€çŠ¶æ€è¯„ä¼°åŠ¨ä½œ
                    q1_new = self.q1(global_states, new_joint_actions)
                    q2_new = self.q2(global_states, new_joint_actions)
                    q_new = torch.min(q1_new, q2_new)
                    
                    # ğŸ”§ è”åˆç­–ç•¥ç†µé¡¹ï¼šå¯¹è·Ÿéšè€…ç»´åº¦æ±‚å’Œï¼ˆä¸ target_entropy å®šä¹‰ä¸€è‡´ï¼‰
                    log_probs_joint = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                    actor_loss = (self.alpha * log_probs_joint - q_new).mean()
                
                self.scaler.scale(actor_loss).backward()
                self.scaler.unscale_(self.actor_optimizer)
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                self.scaler.step(self.actor_optimizer)
            else:
                new_actions, log_probs, _ = self.actor(follower_states)
                new_joint_actions = new_actions.view(batch_size, -1)
                
                q1_new = self.q1(global_states, new_joint_actions)
                q2_new = self.q2(global_states, new_joint_actions)
                q_new = torch.min(q1_new, q2_new)
                
                log_probs_joint = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                actor_loss = (self.alpha * log_probs_joint - q_new).mean()
                
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                self.actor_optimizer.step()
            
            # ========== Alpha æ›´æ–° ==========
            if self.auto_entropy:
                self.alpha_optimizer.zero_grad(set_to_none=True)

                # ğŸ”§ alpha æ›´æ–°ä¹Ÿåº”åŸºäºâ€œè”åˆåŠ¨ä½œâ€çš„ log-probï¼ˆå¯¹è·Ÿéšè€…æ±‚å’Œåå†å¯¹ batch æ±‚å‡å€¼ï¼‰
                log_probs_joint_detached = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1).detach()
                mean_log_prob = log_probs_joint_detached.mean()

                alpha_loss = -(self.log_alpha * (mean_log_prob + self.target_entropy))
                alpha_loss.backward()
                self.alpha_optimizer.step()
                self.alpha = self.log_alpha.exp().item()
            
            # ========== AMP Scaler æ›´æ–°ï¼ˆåœ¨æ‰€æœ‰ step ä¹‹åï¼‰==========
            if self.use_amp:
                self.scaler.update()
            
            # ========== è½¯æ›´æ–° ==========
            self._soft_update(self.q1, self.q1_target)
            self._soft_update(self.q2, self.q2_target)
            
            total_q1_loss += q1_loss.item()
            total_q2_loss += q2_loss.item()
            total_actor_loss += actor_loss.item()
        
        self.last_losses = {
            'q1': total_q1_loss / gradient_steps,
            'q2': total_q2_loss / gradient_steps,
            'actor': total_actor_loss / gradient_steps,
            'alpha': self.alpha
        }
        
        return self.last_losses
    
    @torch.no_grad()
    def _soft_update(self, source, target):
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.lerp_(param.data, TAU)
    
    def save(self, path):
        torch.save({
            'actor': self.actor.state_dict(),
            'q1': self.q1.state_dict(),
            'q2': self.q2.state_dict(),
            'q1_target': self.q1_target.state_dict(),
            'q2_target': self.q2_target.state_dict(),
            'log_alpha': self.log_alpha,
            'update_count': self.update_count,
        }, path)
        print(f"âœ… CTDE Model saved to {path}")
    
    def load(self, path):
        checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)
        self.actor.load_state_dict(checkpoint['actor'])
        self.q1.load_state_dict(checkpoint['q1'])
        self.q2.load_state_dict(checkpoint['q2'])
        self.q1_target.load_state_dict(checkpoint['q1_target'])
        self.q2_target.load_state_dict(checkpoint['q2_target'])
        if 'log_alpha' in checkpoint:
            self.log_alpha = checkpoint['log_alpha']
            self.alpha = self.log_alpha.exp().item()
        print(f"âœ… CTDE Model loaded from {path}")


# ä¿ç•™æ—§åç§°ä»¥å…¼å®¹
SACAgent = CTDESACAgent