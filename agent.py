"""
SAC æ™ºèƒ½ä½“ - CTDE æ¶æ„ç‰ˆæœ¬

å…³é”®åŒºåˆ«ï¼š
- Actorï¼šåˆ†æ•£å¼ï¼Œåªç”¨æœ¬åœ°è§‚æµ‹
- Criticï¼šé›†ä¸­å¼ï¼Œç”¨å…¨å±€çŠ¶æ€ + è”åˆåŠ¨ä½œ
"""
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

from config import (
    DEVICE, STATE_DIM, HIDDEN_DIM, ACTION_DIM, NUM_AGENTS,
    LEARNING_RATE, ALPHA_LR, GAMMA, TAU, BATCH_SIZE,
    INIT_ALPHA, GRADIENT_STEPS, NUM_FOLLOWERS, GLOBAL_STATE_DIM,
    POLICY_DELAY, TARGET_UPDATE_INTERVAL,
    TARGET_ENTROPY_RATIO,

    # MAPPO/PPO
    PPO_LR, PPO_CLIP_EPS, PPO_EPOCHS, PPO_ROLLOUT_STEPS, PPO_MINIBATCH_SIZE,
    PPO_GAE_LAMBDA, PPO_VALUE_COEF, PPO_ENTROPY_COEF, PPO_MAX_GRAD_NORM, PPO_TARGET_KL,
)
from buffer import CTDEReplayBuffer
from networks import GaussianActor, SoftQNetwork, ValueNetwork


class CTDESACAgent:
    """
    CTDE SAC æ™ºèƒ½ä½“
    
    Centralized Training:
    - Critic ä½¿ç”¨å…¨å±€çŠ¶æ€ + æ‰€æœ‰æ™ºèƒ½ä½“çš„è”åˆåŠ¨ä½œ
    
    Decentralized Execution:
    - Actor åªä½¿ç”¨å•ä¸ªæ™ºèƒ½ä½“çš„æœ¬åœ°è§‚æµ‹
    """
    
    def __init__(self, topology, auto_entropy=True, use_amp=True):
        self.topology = topology
        self.num_followers = topology.num_followers
        self.num_agents = topology.num_agents
        self.auto_entropy = auto_entropy
        self.use_amp = use_amp and torch.cuda.is_available()
        
        # åˆ†æ•£å¼ Actorï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“å…±äº«å‚æ•°ï¼‰
        self.actor = GaussianActor(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        
        # é›†ä¸­å¼ Criticï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰
        self.q1 = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2 = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q1_target = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2_target = SoftQNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)
        
        self.q1_target.load_state_dict(self.q1.state_dict())
        self.q2_target.load_state_dict(self.q2.state_dict())
        
        for param in self.q1_target.parameters():
            param.requires_grad = False
        for param in self.q2_target.parameters():
            param.requires_grad = False
        
        if self.use_amp:
            # å…¼å®¹ PyTorch 2.xï¼šä¼˜å…ˆç”¨ torch.ampï¼Œé¿å… FutureWarning
            try:
                from torch.amp import GradScaler
                self.scaler = GradScaler('cuda')
                self._autocast = lambda: torch.amp.autocast('cuda')
            except Exception:
                from torch.cuda.amp import GradScaler
                self.scaler = GradScaler()
                self._autocast = lambda: torch.cuda.amp.autocast()
            print("ğŸš€ AMP (æ··åˆç²¾åº¦è®­ç»ƒ) å·²å¯ç”¨ - CTDE æ¶æ„")
        else:
            self.scaler = None
            self._autocast = None
        
        # æ¸©åº¦å‚æ•°ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“å…±äº«ï¼‰
        # å¯¹äºå¤šæ™ºèƒ½ä½“è”åˆåŠ¨ä½œç©ºé—´ï¼Œtarget_entropy åº”è€ƒè™‘æ‰€æœ‰æ™ºèƒ½ä½“
        # å…è®¸ç”¨ TARGET_ENTROPY_RATIO ç¼©æ”¾ç†µç›®æ ‡ï¼šæ¯”ä¾‹è¶Šå° -> æ¢ç´¢è¶Šå¼±ï¼Œå­¦ä¹ æ›´ç¨³
        self.target_entropy = -float(ACTION_DIM * self.num_followers) * float(TARGET_ENTROPY_RATIO)
        self.log_alpha = torch.tensor(np.log(INIT_ALPHA), requires_grad=True, device=DEVICE)
        self.alpha = self.log_alpha.exp().item()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE)
        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=LEARNING_RATE)
        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=LEARNING_RATE)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=ALPHA_LR)
        
        # CTDE ç¼“å†²åŒº
        self.buffer = CTDEReplayBuffer(num_agents=NUM_AGENTS)
        
        self.last_losses = {'q1': 0, 'q2': 0, 'actor': 0, 'alpha': INIT_ALPHA}
        self.update_count = 0
        
        print(f"ğŸ“Š CTDE Agent initialized:")
        print(f"   Actor input: Local state ({STATE_DIM})")
        print(f"   Critic input: Global state ({GLOBAL_STATE_DIM}) + Joint action ({NUM_FOLLOWERS * ACTION_DIM})")
    
    @torch.inference_mode()
    def select_action(self, local_states, deterministic=False):
        """
        åˆ†æ•£å¼åŠ¨ä½œé€‰æ‹©ï¼ˆåªç”¨æœ¬åœ°è§‚æµ‹ï¼‰
        
        Args:
            local_states: (batch, num_agents, state_dim) æˆ– (num_agents, state_dim)
        """
        is_batched = local_states.dim() == 3
        
        if is_batched:
            batch_size = local_states.shape[0]
            # åªå¤„ç†è·Ÿéšè€…
            follower_states = local_states[:, 1:, :]
            flat_states = follower_states.reshape(-1, STATE_DIM)
            
            if self.use_amp:
                with self._autocast():
                    action, _, _ = self.actor(flat_states, deterministic=deterministic)
            else:
                action, _, _ = self.actor(flat_states, deterministic=deterministic)
            
            action = action.view(batch_size, self.num_followers, ACTION_DIM)
        else:
            follower_states = local_states[1:, :]
            
            if self.use_amp:
                with self._autocast():
                    action, _, _ = self.actor(follower_states, deterministic=deterministic)
            else:
                action, _, _ = self.actor(follower_states, deterministic=deterministic)
        
        return action.float()
    
    def store_transitions_batch(self, local_states, global_states, actions, rewards, 
                                next_local_states, next_global_states, dones):
        """æ‰¹é‡å­˜å‚¨ï¼ˆåŒ…å«å…¨å±€çŠ¶æ€ï¼‰"""
        self.buffer.push_batch(local_states, global_states, actions, rewards, 
                               next_local_states, next_global_states, dones)
    
    def update(self, batch_size=BATCH_SIZE, gradient_steps=GRADIENT_STEPS):
        """æ›´æ–°ç½‘ç»œï¼ˆCTDE æ–¹å¼ï¼‰"""
        if not self.buffer.is_ready(batch_size):
            return {}
        
        total_q1_loss = 0.0
        total_q2_loss = 0.0
        total_actor_loss = 0.0

        # è¯Šæ–­ç»Ÿè®¡ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ Q å‘æ•£/Î± è¿‡å¤§/ç†µå¡Œé™·ç­‰ï¼‰
        total_q1_mean = 0.0
        total_q2_mean = 0.0
        total_target_q_mean = 0.0
        total_logp_joint = 0.0
        total_entropy_joint = 0.0
        total_alpha_loss = 0.0
        policy_updates = 0
        
        for _ in range(gradient_steps):
            self.update_count += 1
            
            # é‡‡æ ·
            (local_states, global_states, actions, rewards, 
             next_local_states, next_global_states, dones) = self.buffer.sample(batch_size)
            
            # å‡†å¤‡æ•°æ®
            follower_states = local_states[:, 1:, :].reshape(-1, STATE_DIM)
            follower_next_states = next_local_states[:, 1:, :].reshape(-1, STATE_DIM)
            joint_actions = actions.view(batch_size, -1)  # (batch, num_followers * action_dim)
            
            # ========== Critic æ›´æ–°ï¼ˆä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰==========
            with torch.no_grad():
                # ä½¿ç”¨ Actor ç”Ÿæˆä¸‹ä¸€æ­¥åŠ¨ä½œ
                if self.use_amp:
                    with self._autocast():
                        next_actions, next_log_probs, _ = self.actor(follower_next_states)
                else:
                    next_actions, next_log_probs, _ = self.actor(follower_next_states)
                
                # é‡å¡‘ä¸ºè”åˆåŠ¨ä½œ
                next_joint_actions = next_actions.view(batch_size, -1)

                # ğŸ”§ å¤šæ™ºèƒ½ä½“ç†µé¡¹ï¼šåº”å¯¹è”åˆç­–ç•¥çš„ log-prob åšâ€œæ±‚å’Œâ€è€Œä¸æ˜¯å‡å€¼
                # next_log_probs: (batch*num_followers, 1) -> (batch, num_followers, 1) -> (batch, 1)
                next_log_probs_joint = next_log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                
                # ä½¿ç”¨å…¨å±€çŠ¶æ€è®¡ç®— Q å€¼
                q1_next = self.q1_target(next_global_states, next_joint_actions)
                q2_next = self.q2_target(next_global_states, next_joint_actions)
                q_next = torch.min(q1_next, q2_next)
                
                target_q = rewards.unsqueeze(1) + GAMMA * (1 - dones.unsqueeze(1)) * (q_next - self.alpha * next_log_probs_joint)
                target_q = target_q.float()
            
            # ========== Critic æ›´æ–°ï¼ˆåˆå¹¶åä¼ ï¼Œå‡å°‘ Python/ä¼˜åŒ–å™¨å¼€é”€ï¼‰ ==========
            self.q1_optimizer.zero_grad(set_to_none=True)
            self.q2_optimizer.zero_grad(set_to_none=True)

            if self.use_amp:
                with self._autocast():
                    q1_curr = self.q1(global_states, joint_actions)
                    q2_curr = self.q2(global_states, joint_actions)
                    q1_loss = F.mse_loss(q1_curr.float(), target_q)
                    q2_loss = F.mse_loss(q2_curr.float(), target_q)
                    critic_loss = q1_loss + q2_loss

                self.scaler.scale(critic_loss).backward()

                # åˆ†åˆ« unscale + clipï¼ˆä¸¤å¥—å‚æ•°å„è‡ªè£å‰ªï¼‰
                self.scaler.unscale_(self.q1_optimizer)
                self.scaler.unscale_(self.q2_optimizer)
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)

                self.scaler.step(self.q1_optimizer)
                self.scaler.step(self.q2_optimizer)
            else:
                q1_curr = self.q1(global_states, joint_actions)
                q2_curr = self.q2(global_states, joint_actions)
                q1_loss = F.mse_loss(q1_curr, target_q)
                q2_loss = F.mse_loss(q2_curr, target_q)
                critic_loss = q1_loss + q2_loss

                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)
                self.q1_optimizer.step()
                self.q2_optimizer.step()

            # ========== Actor/Alpha æ›´æ–°ï¼ˆPolicy Delayï¼šä¸å¿…æ¯ä¸ª step éƒ½åšåä¼ ï¼‰ ==========
            do_policy_update = (self.update_count % max(1, POLICY_DELAY) == 0)
            actor_loss = torch.tensor(0.0, device=DEVICE)

            if do_policy_update:
                self.actor_optimizer.zero_grad(set_to_none=True)
                if self.use_amp:
                    with self._autocast():
                        new_actions, log_probs, _ = self.actor(follower_states)
                        new_joint_actions = new_actions.view(batch_size, -1)

                        # ä½¿ç”¨å…¨å±€çŠ¶æ€è¯„ä¼°åŠ¨ä½œ
                        q1_new = self.q1(global_states, new_joint_actions)
                        q2_new = self.q2(global_states, new_joint_actions)
                        q_new = torch.min(q1_new, q2_new)

                        # ğŸ”§ è”åˆç­–ç•¥ç†µé¡¹ï¼šå¯¹è·Ÿéšè€…ç»´åº¦æ±‚å’Œï¼ˆä¸ target_entropy å®šä¹‰ä¸€è‡´ï¼‰
                        log_probs_joint = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                        actor_loss = (self.alpha * log_probs_joint - q_new).mean()

                    self.scaler.scale(actor_loss).backward()
                    self.scaler.unscale_(self.actor_optimizer)
                    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                    self.scaler.step(self.actor_optimizer)
                else:
                    new_actions, log_probs, _ = self.actor(follower_states)
                    new_joint_actions = new_actions.view(batch_size, -1)

                    q1_new = self.q1(global_states, new_joint_actions)
                    q2_new = self.q2(global_states, new_joint_actions)
                    q_new = torch.min(q1_new, q2_new)

                    log_probs_joint = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1)
                    actor_loss = (self.alpha * log_probs_joint - q_new).mean()

                    actor_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                    self.actor_optimizer.step()

                # ========== Alpha æ›´æ–°ï¼ˆåŒæ ·å»¶ååˆ° policy updateï¼‰ ==========
                if self.auto_entropy:
                    self.alpha_optimizer.zero_grad(set_to_none=True)

                    log_probs_joint_detached = log_probs.view(batch_size, self.num_followers, 1).sum(dim=1).detach()
                    mean_log_prob = log_probs_joint_detached.mean()

                    alpha_loss = -(self.log_alpha * (mean_log_prob + self.target_entropy))
                    alpha_loss.backward()
                    self.alpha_optimizer.step()
                    self.alpha = self.log_alpha.exp().item()
            
            # ========== AMP Scaler æ›´æ–°ï¼ˆåœ¨æ‰€æœ‰ step ä¹‹åï¼‰==========
            if self.use_amp:
                self.scaler.update()

            # ========== Target è½¯æ›´æ–°ï¼ˆé™ä½é¢‘ç‡ï¼Œå‡å°‘å‚æ•°æ‹·è´å¼€é”€ï¼‰ ==========
            if self.update_count % max(1, TARGET_UPDATE_INTERVAL) == 0:
                # ç­‰æ•ˆ tauï¼šinterval æ¬¡è¿ç»­ lerp çš„åˆæˆ
                tau_eff = 1.0 - (1.0 - TAU) ** max(1, TARGET_UPDATE_INTERVAL)
                self._soft_update(self.q1, self.q1_target, tau=tau_eff)
                self._soft_update(self.q2, self.q2_target, tau=tau_eff)
            
            total_q1_loss += float(q1_loss.item())
            total_q2_loss += float(q2_loss.item())
            total_actor_loss += float(actor_loss.item())

            # critic ä¾§çš„æ•°å€¼å°ºåº¦ï¼ˆtarget/Q çš„å‡å€¼å¸¸èƒ½å¿«é€Ÿæš´éœ²å‘æ•£ï¼‰
            total_q1_mean += float(q1_curr.detach().mean().item())
            total_q2_mean += float(q2_curr.detach().mean().item())
            total_target_q_mean += float(target_q.detach().mean().item())

            if do_policy_update:
                policy_updates += 1
                total_logp_joint += float(log_probs_joint.detach().mean().item())
                total_entropy_joint += float((-log_probs_joint).detach().mean().item())
                if self.auto_entropy:
                    total_alpha_loss += float(alpha_loss.detach().item())
        
        # policy ç›¸å…³ç»Ÿè®¡å¯èƒ½ä¸æ˜¯æ¯ä¸ª gradient step éƒ½æ›´æ–°
        pol_denom = max(1, int(policy_updates))

        self.last_losses = {
            'q1': total_q1_loss / float(gradient_steps),
            'q2': total_q2_loss / float(gradient_steps),
            'actor': total_actor_loss / float(gradient_steps),
            'alpha': float(self.alpha),

            # è¯Šæ–­é¡¹ï¼ˆå¯ç”¨äº Dashboard/æ—¥å¿—ï¼‰
            'q1_mean': total_q1_mean / float(gradient_steps),
            'q2_mean': total_q2_mean / float(gradient_steps),
            'target_q_mean': total_target_q_mean / float(gradient_steps),
            'logp_joint': total_logp_joint / float(pol_denom),
            'entropy_joint': total_entropy_joint / float(pol_denom),
            'alpha_loss': total_alpha_loss / float(pol_denom) if self.auto_entropy else float('nan'),
            'policy_updates': int(policy_updates),
        }
        
        return self.last_losses
    
    @torch.no_grad()
    def _soft_update(self, source, target, tau: float = TAU):
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.lerp_(param.data, tau)
    
    def save(self, path):
        torch.save({
            'actor': self.actor.state_dict(),
            'q1': self.q1.state_dict(),
            'q2': self.q2.state_dict(),
            'q1_target': self.q1_target.state_dict(),
            'q2_target': self.q2_target.state_dict(),
            'log_alpha': self.log_alpha,
            'update_count': self.update_count,
        }, path)
        print(f"âœ… CTDE Model saved to {path}")
    
    def load(self, path):
        checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)
        self.actor.load_state_dict(checkpoint['actor'])
        self.q1.load_state_dict(checkpoint['q1'])
        self.q2.load_state_dict(checkpoint['q2'])
        self.q1_target.load_state_dict(checkpoint['q1_target'])
        self.q2_target.load_state_dict(checkpoint['q2_target'])

        # ğŸ”§ ä¸è¦ç”¨â€œé‡æ–°èµ‹å€¼â€çš„æ–¹å¼æ›¿æ¢ self.log_alphaï¼›å¦åˆ™ alpha_optimizer ä»æŒ‡å‘æ—§å‚æ•°ï¼ŒÎ± æ›´æ–°ä¼šå¤±æ•ˆ
        if 'log_alpha' in checkpoint:
            ckpt_log_alpha = checkpoint['log_alpha']
            if isinstance(ckpt_log_alpha, torch.Tensor):
                ckpt_log_alpha = ckpt_log_alpha.to(device=DEVICE, dtype=self.log_alpha.dtype)
                self.log_alpha.data.copy_(ckpt_log_alpha)
            else:
                self.log_alpha.data.fill_(float(ckpt_log_alpha))
            self.alpha = self.log_alpha.exp().item()

        if 'update_count' in checkpoint:
            self.update_count = int(checkpoint['update_count'])

        print(f"âœ… CTDE Model loaded from {path}")




class _PPOBuffer:
    """On-policy rollout bufferï¼ˆæŒ‰æ—¶é—´æ­¥å­˜å‚¨ï¼Œæ”¯æŒå¹¶è¡Œç¯å¢ƒï¼‰ã€‚"""

    def __init__(self, rollout_steps: int = PPO_ROLLOUT_STEPS):
        self.rollout_steps = int(rollout_steps)
        self.clear()

    def clear(self):
        self.follower_states = []     # list[(E, F, STATE_DIM)]
        self.global_states = []       # list[(E, G)]
        self.actions = []             # list[(E, F, ACTION_DIM)]
        self.logp_joint = []          # list[(E, 1)]
        self.values = []              # list[(E, 1)]
        self.rewards = []             # list[(E,)]
        self.dones = []               # list[(E,)] bool

    def add(self, follower_states, global_states, actions, logp_joint, values, rewards, dones):
        self.follower_states.append(follower_states)
        self.global_states.append(global_states)
        self.actions.append(actions)
        self.logp_joint.append(logp_joint)
        self.values.append(values)
        self.rewards.append(rewards)
        self.dones.append(dones)

    def __len__(self):
        return len(self.rewards)

    def is_full(self):
        return len(self) >= self.rollout_steps


class CTDEMAPPOAgent:
    """CTDE-MAPPOï¼šcentralized value + decentralized shared policyï¼ˆfactorized followersï¼‰ã€‚"""

    def __init__(self, topology, use_amp: bool = False):
        self.topology = topology
        self.num_followers = topology.num_followers
        self.num_agents = topology.num_agents
        self.use_amp = False  # PPO è¿™é‡Œå…ˆä¸èµ° AMPï¼Œå‡å°‘æ•°å€¼/è°ƒè¯•å¤æ‚åº¦

        self.actor = GaussianActor(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.value_net = ValueNetwork(GLOBAL_STATE_DIM, HIDDEN_DIM).to(DEVICE)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=PPO_LR)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=PPO_LR)

        self.buffer = _PPOBuffer(rollout_steps=PPO_ROLLOUT_STEPS)

        self.last_losses = {}

        print("ğŸ“Š CTDE MAPPO Agent initialized:")
        print(f"   Actor input: Local state ({STATE_DIM})")
        print(f"   Value input: Global state ({GLOBAL_STATE_DIM})")
        print(f"   Rollout steps: {PPO_ROLLOUT_STEPS} | Epochs: {PPO_EPOCHS} | Minibatch: {PPO_MINIBATCH_SIZE}")

    @torch.inference_mode()
    def select_action(self, local_states, deterministic=False):
        """ä¸ SAC Agent å¯¹é½çš„æ¥å£ï¼šåªè¿”å›åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°/å¯è§†åŒ–ï¼‰ã€‚

        æ”¯æŒï¼š
        - batched: (E, A, S) -> è¿”å› (E, F, A)
        - single:  (A, S)    -> è¿”å› (F, A)
        """
        if local_states.dim() == 2:
            actions, _, _ = self.act(local_states.unsqueeze(0), None, deterministic=deterministic)
            return actions[0]
        actions, _, _ = self.act(local_states, None, deterministic=deterministic)
        return actions

    @torch.inference_mode()
    def act(self, local_states, global_states=None, deterministic=False):
        """é‡‡æ ·åŠ¨ä½œï¼Œå¹¶è¿”å› joint log-prob å’Œ valueï¼ˆç”¨äº PPO rollout æ”¶é›†ï¼‰ã€‚

        Args:
            local_states: (E, A, STATE_DIM)
            global_states: (E, G) or None

        Returns:
            actions: (E, F, ACTION_DIM)
            logp_joint: (E, 1)
            values: (E, 1)ï¼ˆè‹¥ global_states=None åˆ™ä¸º NaNï¼‰
        """
        assert local_states.dim() == 3, "MAPPO è®­ç»ƒéœ€è¦ batched local_states"
        E = local_states.shape[0]

        follower_states = local_states[:, 1:, :].reshape(-1, STATE_DIM)
        act_flat, logp_flat, _ = self.actor(follower_states, deterministic=deterministic)
        actions = act_flat.view(E, self.num_followers, ACTION_DIM).float()

        # factorized policy -> joint log-prob = sum(logp_i)
        # æ³¨æ„ï¼šactor åœ¨ deterministic=True æ—¶ä¼šè¿”å› logp=Noneï¼ˆå› ä¸ºæ²¡æœ‰é‡‡æ ·åŠ¨ä½œï¼‰ã€‚
        # å¯¹è¯„ä¼°/å¯è§†åŒ–è€Œè¨€æˆ‘ä»¬åªéœ€è¦åŠ¨ä½œï¼Œæ‰€ä»¥è¿™é‡Œç»™ä¸€ä¸ªå ä½ 0ï¼Œé¿å… None ä¼ æ’­ã€‚
        if logp_flat is None:
            logp_joint = torch.zeros((E, 1), device=DEVICE)
        else:
            logp_joint = logp_flat.view(E, self.num_followers, 1).sum(dim=1)

        if global_states is None:
            values = torch.full((E, 1), float('nan'), device=DEVICE)
        else:
            values = self.value_net(global_states).float()

        return actions, logp_joint, values

    def store_rollout_step(self, local_states, global_states, actions, logp_joint, values, rewards, dones):
        follower_states = local_states[:, 1:, :].detach()
        self.buffer.add(
            follower_states.float(),
            global_states.detach().float(),
            actions.detach().float(),
            logp_joint.detach().float(),
            values.detach().float(),
            rewards.detach().float(),
            dones.detach(),
        )

    def _compute_gae(self, rewards, dones, values, last_value):
        """GAE(lambda)ã€‚

        Args:
            rewards: (T, E)
            dones: (T, E) bool
            values: (T, E)
            last_value: (E,)
        Returns:
            advantages: (T, E)
            returns: (T, E)
        """
        T, E = rewards.shape
        advantages = torch.zeros(T, E, device=DEVICE)
        gae = torch.zeros(E, device=DEVICE)

        for t in reversed(range(T)):
            nonterminal = (~dones[t]).float()
            next_value = last_value if t == T - 1 else values[t + 1]
            delta = rewards[t] + GAMMA * next_value * nonterminal - values[t]
            gae = delta + GAMMA * PPO_GAE_LAMBDA * nonterminal * gae
            advantages[t] = gae

        returns = advantages + values
        return advantages, returns

    def update(self, next_global_states=None, next_dones=None):
        """ç”¨ buffer ä¸­çš„ on-policy rollout åš PPO æ›´æ–°ã€‚"""
        if len(self.buffer) == 0:
            return {}

        # stack: list[T] -> (T, ...)
        follower_states = torch.stack(self.buffer.follower_states, dim=0)  # (T,E,F,S)
        global_states = torch.stack(self.buffer.global_states, dim=0)      # (T,E,G)
        actions = torch.stack(self.buffer.actions, dim=0)                  # (T,E,F,A)
        old_logp_joint = torch.stack(self.buffer.logp_joint, dim=0)        # (T,E,1)
        values = torch.stack(self.buffer.values, dim=0)                    # (T,E,1)
        rewards = torch.stack(self.buffer.rewards, dim=0)                  # (T,E)
        dones = torch.stack(self.buffer.dones, dim=0)                      # (T,E)

        T, E = rewards.shape

        with torch.no_grad():
            if next_global_states is None:
                last_value = torch.zeros(E, device=DEVICE)
            else:
                lv = self.value_net(next_global_states).squeeze(-1)
                if next_dones is not None:
                    lv = lv * (~next_dones).float()
                last_value = lv

            adv, ret = self._compute_gae(
                rewards=rewards,
                dones=dones,
                values=values.squeeze(-1),
                last_value=last_value,
            )

            # advantage normalizationï¼ˆPPO å¸¸ç”¨ï¼‰
            adv_flat = adv.reshape(-1)
            adv = (adv - adv_flat.mean()) / (adv_flat.std(unbiased=False) + 1e-8)

        # flatten time/env ç»´
        N = T * E
        follower_states_env = follower_states.reshape(N, self.num_followers, STATE_DIM)
        global_states_env = global_states.reshape(N, -1)
        actions_env = actions.reshape(N, self.num_followers, ACTION_DIM)
        old_logp_env = old_logp_joint.reshape(N, 1)
        adv_env = adv.reshape(N, 1)
        ret_env = ret.reshape(N, 1)

        mb_size = int(PPO_MINIBATCH_SIZE)
        clip_eps = float(PPO_CLIP_EPS)

        # logging accumulators
        pol_losses = []
        v_losses = []
        entropies = []
        kls = []
        clipfracs = []

        for _ in range(int(PPO_EPOCHS)):
            perm = torch.randperm(N, device=DEVICE)
            for start in range(0, N, mb_size):
                idx = perm[start:start + mb_size]

                mb_gs = global_states_env[idx]
                mb_fs = follower_states_env[idx]            # (B,F,S)
                mb_act = actions_env[idx]                   # (B,F,A)
                mb_old_logp = old_logp_env[idx]
                mb_adv = adv_env[idx]
                mb_ret = ret_env[idx]

                # ========== policy loss ==========
                fs_flat = mb_fs.reshape(-1, STATE_DIM)
                act_flat = mb_act.reshape(-1, ACTION_DIM)

                new_logp_flat = self.actor.evaluate_actions(fs_flat, act_flat)  # (B*F,1)
                new_logp_joint = new_logp_flat.view(-1, self.num_followers, 1).sum(dim=1)

                ratio = torch.exp(new_logp_joint - mb_old_logp)
                surr1 = ratio * mb_adv
                surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * mb_adv
                policy_loss = -(torch.min(surr1, surr2)).mean()

                # entropy è¿‘ä¼¼ï¼šç”¨ -logp ä½œä¸º proxyï¼ˆä¸ SAC çš„ joint entropy ç»Ÿè®¡ä¸€è‡´ï¼‰
                entropy_joint = (-new_logp_joint).mean()

                # approx KL
                approx_kl = (mb_old_logp - new_logp_joint).mean()
                clipfrac = (torch.abs(ratio - 1.0) > clip_eps).float().mean()

                actor_loss = policy_loss - float(PPO_ENTROPY_COEF) * entropy_joint

                self.actor_optimizer.zero_grad(set_to_none=True)
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), float(PPO_MAX_GRAD_NORM))
                self.actor_optimizer.step()

                # ========== value loss ==========
                v_pred = self.value_net(mb_gs)
                value_loss = F.mse_loss(v_pred, mb_ret)

                self.value_optimizer.zero_grad(set_to_none=True)
                (float(PPO_VALUE_COEF) * value_loss).backward()
                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), float(PPO_MAX_GRAD_NORM))
                self.value_optimizer.step()

                pol_losses.append(float(policy_loss.detach().item()))
                v_losses.append(float(value_loss.detach().item()))
                entropies.append(float(entropy_joint.detach().item()))
                kls.append(float(approx_kl.detach().item()))
                clipfracs.append(float(clipfrac.detach().item()))

                # optional early stop by KL
                if float(PPO_TARGET_KL) > 0 and float(approx_kl.detach().item()) > 1.5 * float(PPO_TARGET_KL):
                    break

        self.buffer.clear()

        self.last_losses = {
            'policy': float(np.mean(pol_losses)) if pol_losses else float('nan'),
            'value': float(np.mean(v_losses)) if v_losses else float('nan'),
            'entropy_joint': float(np.mean(entropies)) if entropies else float('nan'),
            'kl': float(np.mean(kls)) if kls else float('nan'),
            'clipfrac': float(np.mean(clipfracs)) if clipfracs else float('nan'),
        }
        return self.last_losses

    def save(self, path):
        torch.save({
            'actor': self.actor.state_dict(),
            'value': self.value_net.state_dict(),
        }, path)
        print(f"âœ… CTDE MAPPO Model saved to {path}")

    def load(self, path):
        checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)
        self.actor.load_state_dict(checkpoint['actor'])
        self.value_net.load_state_dict(checkpoint['value'])
        print(f"âœ… CTDE MAPPO Model loaded from {path}")


# ä¿ç•™æ—§åç§°ä»¥å…¼å®¹
SACAgent = CTDESACAgent
MAPPOAgent = CTDEMAPPOAgent
