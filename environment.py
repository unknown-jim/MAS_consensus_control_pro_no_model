"""
é¢†å¯¼è€…-è·Ÿéšè€…å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç¯å¢ƒ - CTDE ç‰ˆæœ¬ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰

æ–°å¢åŠŸèƒ½ï¼š
- æ¯ä¸ª episode éšæœºåŒ–é¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°ï¼ˆæŒ¯å¹…ã€è§’é¢‘ç‡ã€ç›¸ä½ï¼‰
- æ¯ä¸ª episode éšæœºåŒ–è·Ÿéšè€…åˆå§‹çŠ¶æ€åˆ†å¸ƒ
- æ¯ä¸ª episode éšæœºåŒ–é€šä¿¡æ‹“æ‰‘ï¼ˆä¿è¯è¿é€šæ€§ï¼‰
"""
import torch

from config import (
    DEVICE, DT,
    LOCAL_OBS_DIM, NEIGHBOR_OBS_DIM, MAX_NEIGHBORS, STATE_DIM,
    SELF_ROLE_DIM, NEIGHBOR_ROLE_DIM, NEIGHBOR_FEAT_DIM,
    COMM_PENALTY, THRESHOLD_MIN, THRESHOLD_MAX,
    TRACKING_PENALTY_SCALE, TRACKING_PENALTY_MAX, COMM_WEIGHT_DECAY,
    IMPROVEMENT_SCALE, IMPROVEMENT_CLIP,
    LEADER_AMPLITUDE, LEADER_OMEGA, LEADER_PHASE,
    POS_LIMIT, VEL_LIMIT,
    REWARD_MIN, REWARD_MAX, USE_SOFT_REWARD_SCALING,
    TH_SCALE, V_SCALE, NUM_AGENTS, GLOBAL_STATE_DIM,
    # éšæœºåŒ–å‚æ•°
    RANDOMIZE_LEADER, RANDOMIZE_FOLLOWER, RANDOMIZE_TOPOLOGY,
    LEADER_AMPLITUDE_RANGE, LEADER_OMEGA_RANGE, LEADER_PHASE_RANGE,
    FOLLOWER_INIT_POS_STD_RANGE, FOLLOWER_INIT_VEL_STD_RANGE,
    LEADER_TRAJECTORY_TYPES
)


class BatchedModelFreeEnv:
    """
    æ— æ¨¡å‹æ‰¹é‡ç¯å¢ƒ - CTDE ç‰ˆæœ¬ï¼ˆéšæœºåˆå§‹åŒ– + å¤šè½¨è¿¹ç±»å‹ + æ‹“æ‰‘éšæœºåŒ–ï¼‰
    
    æ–°å¢åŠŸèƒ½ï¼š
    - get_global_state(): è¿”å›å…¨å±€çŠ¶æ€ç”¨äºé›†ä¸­å¼ Critic è®­ç»ƒ
    - æ¯ä¸ª episode éšæœºåŒ–é¢†å¯¼è€…åŠ¨åŠ›å­¦å’Œè·Ÿéšè€…åˆå§‹çŠ¶æ€
    - æ”¯æŒå¤šç§é¢†å¯¼è€…è½¨è¿¹ç±»å‹ï¼šsine, cosine, mixed, chirp
    - æ¯ä¸ª episode éšæœºåŒ–é€šä¿¡æ‹“æ‰‘ï¼ˆä¿è¯è¿é€šæ€§ï¼‰
    """
    
    def __init__(self, topology, num_envs=64):
        self.topology = topology
        self.num_envs = num_envs
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
        self.leader_id = topology.leader_id
        
        # é¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°ï¼ˆåŸºå‡†å€¼ï¼Œä¼šåœ¨ reset æ—¶éšæœºåŒ–ï¼‰
        self.leader_amplitude_base = LEADER_AMPLITUDE
        self.leader_omega_base = LEADER_OMEGA
        self.leader_phase_base = LEADER_PHASE
        
        # ğŸ”§ ä¸ºæ¯ä¸ªç¯å¢ƒå­˜å‚¨ç‹¬ç«‹çš„é¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°
        self.leader_amplitude = torch.full((num_envs,), LEADER_AMPLITUDE, device=DEVICE)
        self.leader_omega = torch.full((num_envs,), LEADER_OMEGA, device=DEVICE)
        self.leader_phase = torch.full((num_envs,), LEADER_PHASE, device=DEVICE)
        
        # ğŸ”§ è½¨è¿¹ç±»å‹æ”¯æŒ
        self.trajectory_types = LEADER_TRAJECTORY_TYPES
        self.type_to_id = {t: i for i, t in enumerate(self.trajectory_types)}
        self.id_to_type = {i: t for i, t in enumerate(self.trajectory_types)}
        self.trajectory_type_ids = torch.zeros(num_envs, dtype=torch.long, device=DEVICE)
        
        # éšæœºåŒ–å¼€å…³
        self.randomize_leader = RANDOMIZE_LEADER
        self.randomize_follower = RANDOMIZE_FOLLOWER
        self.randomize_topology = RANDOMIZE_TOPOLOGY
        
        # é¢†å¯¼è€…éšæœºåŒ–èŒƒå›´
        self.amplitude_range = LEADER_AMPLITUDE_RANGE
        self.omega_range = LEADER_OMEGA_RANGE
        self.phase_range = LEADER_PHASE_RANGE
        
        # è·Ÿéšè€…éšæœºåŒ–èŒƒå›´
        self.pos_std_range = FOLLOWER_INIT_POS_STD_RANGE
        self.vel_std_range = FOLLOWER_INIT_VEL_STD_RANGE
        
        self.pos_limit = POS_LIMIT
        self.vel_limit = VEL_LIMIT
        self.reward_min = REWARD_MIN
        self.reward_max = REWARD_MAX
        self.use_soft_scaling = USE_SOFT_REWARD_SCALING
        
        # å¥–åŠ±å‚æ•°
        self.comm_penalty_base = COMM_PENALTY
        self.threshold_min = THRESHOLD_MIN
        self.threshold_max = THRESHOLD_MAX
        self.tracking_penalty_scale = TRACKING_PENALTY_SCALE
        self.tracking_penalty_max = TRACKING_PENALTY_MAX
        self.comm_weight_decay = COMM_WEIGHT_DECAY
        self.improvement_scale = IMPROVEMENT_SCALE
        self.improvement_clip = IMPROVEMENT_CLIP
        
        self.th_scale = TH_SCALE
        self.v_scale = V_SCALE
        
        # é¢„è®¡ç®—é‚»å±…ç´¢å¼•
        self._precompute_neighbor_indices()
        
        # é¢„åˆ†é…çŠ¶æ€å¼ é‡
        self.positions = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.velocities = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        
        # å¹¿æ’­çŠ¶æ€
        self.last_broadcast_pos = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.last_broadcast_vel = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        
        self.t = torch.zeros(num_envs, device=DEVICE)
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒå•ç‹¬å­˜å‚¨ prev_error
        self._prev_error = torch.zeros(num_envs, device=DEVICE)
        self._prev_error_valid = torch.zeros(num_envs, dtype=torch.bool, device=DEVICE)
        
        # é¢„åˆ†é…çŠ¶æ€ç¼“å­˜
        self._state_buffer = torch.zeros(num_envs, self.num_agents, STATE_DIM, device=DEVICE)
        
        # CTDEï¼šé¢„åˆ†é…å…¨å±€çŠ¶æ€ç¼“å­˜
        self._global_state_buffer = torch.zeros(num_envs, GLOBAL_STATE_DIM, device=DEVICE)
        
        self.reset()
    
    def _precompute_neighbor_indices(self, verbose=True):
        """é¢„è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“çš„é‚»å±…ç´¢å¼•"""
        self._neighbor_indices_list = []
        self._neighbor_counts = torch.zeros(self.num_agents, dtype=torch.long, device=DEVICE)
        
        for i in range(self.num_agents):
            can_receive_mask = self.topology.adj_matrix[i, :] > 0
            indices = torch.where(can_receive_mask)[0]
            
            if len(indices) > MAX_NEIGHBORS:
                indices = indices[:MAX_NEIGHBORS]
            
            self._neighbor_indices_list.append(indices)
            self._neighbor_counts[i] = len(indices)
        
        self._padded_neighbor_indices = torch.zeros(
            self.num_agents, MAX_NEIGHBORS, dtype=torch.long, device=DEVICE
        )
        self._neighbor_valid_mask = torch.zeros(
            self.num_agents, MAX_NEIGHBORS, dtype=torch.bool, device=DEVICE
        )
        
        for i, indices in enumerate(self._neighbor_indices_list):
            num_neighbors = len(indices)
            if num_neighbors > 0:
                self._padded_neighbor_indices[i, :num_neighbors] = indices
                self._neighbor_valid_mask[i, :num_neighbors] = True
        
        self._max_actual_neighbors = int(self._neighbor_counts.max().item())
        
        # ğŸ”§ é¢„è®¡ç®—è§’è‰²ä¿¡æ¯
        self._precompute_role_info()
        
        if verbose:
            print(f"ğŸ“Š Precomputed neighbor indices:")
            print(f"   Max neighbors per agent: {self._max_actual_neighbors}")
            print(f"   Neighbor counts: {self._neighbor_counts.tolist()}")
            print(f"   Role encoding: Leader=0, Pinned=1, Normal=2")
    
    def _precompute_role_info(self, verbose=False):
        """
        ğŸ”§ é¢„è®¡ç®—è§’è‰²ä¿¡æ¯ï¼ˆone-hot ç¼–ç ï¼‰
        
        è§’è‰²å®šä¹‰ï¼š
        - 0: é¢†å¯¼è€… (Leader)
        - 1: ç›´æ¥ä¸é¢†å¯¼è€…é€šä¿¡çš„è·Ÿéšè€… (Pinned Follower)
        - 2: æ™®é€šè·Ÿéšè€… (Normal Follower)
        """
        # æ¯ä¸ªæ™ºèƒ½ä½“çš„è§’è‰² ID
        self._role_ids = torch.zeros(self.num_agents, dtype=torch.long, device=DEVICE)
        self._role_ids[0] = 0  # é¢†å¯¼è€…
        
        pinned_set = set(self.topology.pinned_followers)
        for i in range(1, self.num_agents):
            if i in pinned_set:
                self._role_ids[i] = 1  # Pinned follower
            else:
                self._role_ids[i] = 2  # Normal follower
        
        # é¢„è®¡ç®— one-hot ç¼–ç  (num_agents, 3)
        self._role_onehot = torch.zeros(self.num_agents, SELF_ROLE_DIM, device=DEVICE)
        self._role_onehot.scatter_(1, self._role_ids.unsqueeze(1), 1.0)
        
        if verbose:
            print(f"   Pinned followers: {self.topology.pinned_followers}")
            print(f"   Role IDs: {self._role_ids.tolist()}")
    
    def _leader_state_batch(self, t, env_ids=None):
        """
        æ‰¹é‡è®¡ç®—é¢†å¯¼è€…çŠ¶æ€ï¼ˆæ”¯æŒæ¯ä¸ªç¯å¢ƒç‹¬ç«‹çš„åŠ¨åŠ›å­¦å‚æ•°å’Œè½¨è¿¹ç±»å‹ï¼‰
        
        Args:
            t: æ—¶é—´å¼ é‡
            env_ids: ç¯å¢ƒç´¢å¼•ï¼ˆNone è¡¨ç¤ºæ‰€æœ‰ç¯å¢ƒï¼‰
        
        è½¨è¿¹ç±»å‹ï¼š
        - sine: A * sin(Ï‰*t + Ï†)
        - cosine: A * cos(Ï‰*t + Ï†)
        - mixed: A * (sin(Ï‰*t + Ï†) + 0.3*cos(0.5*Ï‰*t))
        - chirp: A * sin((Ï‰ + 0.1*t)*t + Ï†)  å˜é¢‘ä¿¡å·
        """
        if env_ids is None:
            amplitude = self.leader_amplitude
            omega = self.leader_omega
            phase = self.leader_phase
            type_ids = self.trajectory_type_ids
            num_envs = self.num_envs
        else:
            amplitude = self.leader_amplitude[env_ids]
            omega = self.leader_omega[env_ids]
            phase = self.leader_phase[env_ids]
            type_ids = self.trajectory_type_ids[env_ids]
            num_envs = len(env_ids)
        
        pos = torch.zeros(num_envs, device=DEVICE)
        vel = torch.zeros(num_envs, device=DEVICE)
        
        # Sine è½¨è¿¹
        sine_mask = type_ids == self.type_to_id.get('sine', 0)
        if sine_mask.any():
            pos[sine_mask] = amplitude[sine_mask] * torch.sin(omega[sine_mask] * t[sine_mask] + phase[sine_mask])
            vel[sine_mask] = amplitude[sine_mask] * omega[sine_mask] * torch.cos(omega[sine_mask] * t[sine_mask] + phase[sine_mask])
        
        # Cosine è½¨è¿¹
        cosine_mask = type_ids == self.type_to_id.get('cosine', 1)
        if cosine_mask.any():
            pos[cosine_mask] = amplitude[cosine_mask] * torch.cos(omega[cosine_mask] * t[cosine_mask] + phase[cosine_mask])
            vel[cosine_mask] = -amplitude[cosine_mask] * omega[cosine_mask] * torch.sin(omega[cosine_mask] * t[cosine_mask] + phase[cosine_mask])
        
        # Mixed è½¨è¿¹
        mixed_mask = type_ids == self.type_to_id.get('mixed', 2)
        if mixed_mask.any():
            t_m = t[mixed_mask]
            A_m = amplitude[mixed_mask]
            omega_m = omega[mixed_mask]
            phi_m = phase[mixed_mask]
            pos[mixed_mask] = A_m * (torch.sin(omega_m * t_m + phi_m) + 0.3 * torch.cos(0.5 * omega_m * t_m))
            vel[mixed_mask] = A_m * (omega_m * torch.cos(omega_m * t_m + phi_m) - 0.15 * omega_m * torch.sin(0.5 * omega_m * t_m))
        
        # Chirp è½¨è¿¹ï¼ˆå˜é¢‘ä¿¡å·ï¼‰
        chirp_mask = type_ids == self.type_to_id.get('chirp', 3)
        if chirp_mask.any():
            t_c = t[chirp_mask]
            A_c = amplitude[chirp_mask]
            omega_c = omega[chirp_mask]
            phi_c = phase[chirp_mask]
            chirp_rate = 0.1
            inst_phase = (omega_c + chirp_rate * t_c) * t_c + phi_c
            inst_freq = omega_c + 2 * chirp_rate * t_c
            pos[chirp_mask] = A_c * torch.sin(inst_phase)
            vel[chirp_mask] = A_c * inst_freq * torch.cos(inst_phase)
        
        return pos, vel
    
    def _randomize_leader_dynamics(self, env_ids):
        """
        ğŸ”§ éšæœºåŒ–é¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°å’Œè½¨è¿¹ç±»å‹
        
        Args:
            env_ids: éœ€è¦éšæœºåŒ–çš„ç¯å¢ƒç´¢å¼•
        """
        num_envs = len(env_ids)
        
        # éšæœºæŒ¯å¹…
        self.leader_amplitude[env_ids] = torch.empty(num_envs, device=DEVICE).uniform_(
            self.amplitude_range[0], self.amplitude_range[1]
        )
        
        # éšæœºè§’é¢‘ç‡
        self.leader_omega[env_ids] = torch.empty(num_envs, device=DEVICE).uniform_(
            self.omega_range[0], self.omega_range[1]
        )
        
        # éšæœºç›¸ä½
        self.leader_phase[env_ids] = torch.empty(num_envs, device=DEVICE).uniform_(
            self.phase_range[0], self.phase_range[1]
        )
        
        # ğŸ”§ éšæœºè½¨è¿¹ç±»å‹
        import numpy as np
        random_types = np.random.choice(len(self.trajectory_types), size=num_envs)
        self.trajectory_type_ids[env_ids] = torch.tensor(random_types, device=DEVICE)
    
    def _randomize_follower_init(self, env_ids, leader_pos, leader_vel):
        """
        ğŸ”§ éšæœºåŒ–è·Ÿéšè€…åˆå§‹çŠ¶æ€
        
        Args:
            env_ids: éœ€è¦éšæœºåŒ–çš„ç¯å¢ƒç´¢å¼•
            leader_pos: é¢†å¯¼è€…åˆå§‹ä½ç½®
            leader_vel: é¢†å¯¼è€…åˆå§‹é€Ÿåº¦
        """
        num_envs = len(env_ids)
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒéšæœºç”Ÿæˆä½ç½®å’Œé€Ÿåº¦çš„æ ‡å‡†å·®
        pos_std = torch.empty(num_envs, 1, device=DEVICE).uniform_(
            self.pos_std_range[0], self.pos_std_range[1]
        )
        vel_std = torch.empty(num_envs, 1, device=DEVICE).uniform_(
            self.vel_std_range[0], self.vel_std_range[1]
        )
        
        # ç”Ÿæˆéšæœºåç§»
        pos_offset = torch.randn(num_envs, self.num_followers, device=DEVICE) * pos_std
        vel_offset = torch.randn(num_envs, self.num_followers, device=DEVICE) * vel_std
        
        # è®¾ç½®è·Ÿéšè€…åˆå§‹çŠ¶æ€
        self.positions[env_ids, 1:] = leader_pos.unsqueeze(1) + pos_offset
        self.velocities[env_ids, 1:] = leader_vel.unsqueeze(1) + vel_offset
    
    def get_global_state(self):
        """
        CTDEï¼šè·å–å…¨å±€çŠ¶æ€ï¼ˆç”¨äºé›†ä¸­å¼ Criticï¼‰
        
        Returns:
            global_state: (num_envs, global_state_dim)
        """
        self._global_state_buffer[:, 0::2] = self.positions / self.pos_limit
        self._global_state_buffer[:, 1::2] = self.velocities / self.vel_limit
        return self._global_state_buffer.clone()
    
    def reset(self, env_ids=None):
        """
        é‡ç½®ç¯å¢ƒï¼ˆæ”¯æŒéšæœºåˆå§‹åŒ–ï¼‰
        
        Args:
            env_ids: è¦é‡ç½®çš„ç¯å¢ƒç´¢å¼•ï¼ˆNone è¡¨ç¤ºæ‰€æœ‰ç¯å¢ƒï¼‰
        """
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        
        num_reset = len(env_ids) if isinstance(env_ids, torch.Tensor) else self.num_envs
        
        # é‡ç½®æ—¶é—´
        self.t[env_ids] = 0.0
        
        # ğŸ”§ éšæœºåŒ–æ‹“æ‰‘ç»“æ„ï¼ˆä»…åœ¨é‡ç½®æ‰€æœ‰ç¯å¢ƒæ—¶ï¼‰
        if self.randomize_topology and (env_ids is None or len(env_ids) == self.num_envs):
            self._randomize_topology()
        
        # ğŸ”§ éšæœºåŒ–é¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°
        if self.randomize_leader:
            self._randomize_leader_dynamics(env_ids)
        
        # è®¡ç®—é¢†å¯¼è€…åˆå§‹çŠ¶æ€
        leader_pos, leader_vel = self._leader_state_batch(self.t[env_ids], env_ids)
        
        self.positions[env_ids, 0] = leader_pos
        self.velocities[env_ids, 0] = leader_vel
        
        # ğŸ”§ éšæœºåŒ–è·Ÿéšè€…åˆå§‹çŠ¶æ€
        if self.randomize_follower:
            self._randomize_follower_init(env_ids, leader_pos, leader_vel)
        else:
            # ä½¿ç”¨å›ºå®šçš„åˆå§‹åˆ†å¸ƒ
            init_pos_std = 0.5
            init_vel_std = 0.2
            self.positions[env_ids, 1:] = leader_pos.unsqueeze(1) + torch.randn(
                num_reset, self.num_followers, device=DEVICE
            ) * init_pos_std
            self.velocities[env_ids, 1:] = leader_vel.unsqueeze(1) + torch.randn(
                num_reset, self.num_followers, device=DEVICE
            ) * init_vel_std
        
        # é™åˆ¶åœ¨è¾¹ç•Œå†…
        self.positions[env_ids] = torch.clamp(self.positions[env_ids], -self.pos_limit, self.pos_limit)
        self.velocities[env_ids] = torch.clamp(self.velocities[env_ids], -self.vel_limit, self.vel_limit)
        
        # é‡ç½®å¹¿æ’­çŠ¶æ€
        self.last_broadcast_pos[env_ids] = self.positions[env_ids].clone()
        self.last_broadcast_vel[env_ids] = self.velocities[env_ids].clone()
        
        # åªé‡ç½®æŒ‡å®šç¯å¢ƒçš„ prev_error
        self._prev_error[env_ids] = 0.0
        self._prev_error_valid[env_ids] = False
        
        return self._get_state_optimized()
    
    def _randomize_topology(self):
        """
        ğŸ”§ éšæœºåŒ–æ‹“æ‰‘ç»“æ„å¹¶æ›´æ–°ç›¸å…³ç¼“å­˜
        """
        # è°ƒç”¨æ‹“æ‰‘çš„éšæœºåŒ–æ–¹æ³•
        self.topology.randomize()
        
        # é‡æ–°è®¡ç®—é‚»å±…ç´¢å¼•å’Œè§’è‰²ä¿¡æ¯ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        self._precompute_neighbor_indices(verbose=False)
    
    def _get_state_optimized(self):
        """
        è·å–æœ¬åœ°çŠ¶æ€ï¼ˆç”¨äºåˆ†æ•£å¼ Actorï¼‰
        
        çŠ¶æ€ç»“æ„ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“ï¼‰:
        - [0:2] è‡ªèº«ä½ç½®ã€é€Ÿåº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
        - [2:5] è‡ªèº«è§’è‰² one-hot [leader, pinned, normal]
        - [5:5+MAX_NEIGHBORS*5] é‚»å±…æ•°æ®ï¼Œæ¯ä¸ªé‚»å±… 5 ç»´:
            - [0:2] é‚»å±…ä½ç½®ã€é€Ÿåº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
            - [2:5] é‚»å±…è§’è‰² one-hot [leader, pinned, normal]
        """
        self._state_buffer.zero_()
        
        # æœ¬åœ°è§‚æµ‹ï¼šè‡ªèº«ä½ç½®ã€é€Ÿåº¦
        self._state_buffer[:, :, 0] = self.positions / self.pos_limit
        self._state_buffer[:, :, 1] = self.velocities / self.vel_limit
        
        # è‡ªèº«è§’è‰² one-hotï¼ˆå¯¹æ‰€æœ‰ç¯å¢ƒå¹¿æ’­ï¼‰
        # _role_onehot: (num_agents, 3) -> æ‰©å±•åˆ° (num_envs, num_agents, 3)
        self._state_buffer[:, :, LOCAL_OBS_DIM:LOCAL_OBS_DIM + SELF_ROLE_DIM] = self._role_onehot.unsqueeze(0)
        
        # é‚»å±…æ•°æ®èµ·å§‹ä½ç½®
        neighbor_start = LOCAL_OBS_DIM + SELF_ROLE_DIM  # 5
        
        # é‚»å±…å¹¿æ’­çŠ¶æ€ï¼ˆå½’ä¸€åŒ–ï¼‰
        broadcast_pos_norm = self.last_broadcast_pos / self.pos_limit
        broadcast_vel_norm = self.last_broadcast_vel / self.vel_limit
        
        for i in range(self.num_agents):
            num_neighbors = self._neighbor_counts[i].item()
            
            if num_neighbors == 0:
                continue
            
            neighbor_indices = self._neighbor_indices_list[i]
            
            # é‚»å±…ä½ç½®ã€é€Ÿåº¦
            neighbor_pos = broadcast_pos_norm[:, neighbor_indices]
            neighbor_vel = broadcast_vel_norm[:, neighbor_indices]
            
            # é‚»å±…è§’è‰² one-hot
            neighbor_roles = self._role_onehot[neighbor_indices]  # (num_neighbors, 3)
            
            for j in range(num_neighbors):
                base_idx = neighbor_start + j * NEIGHBOR_FEAT_DIM
                # ä½ç½®ã€é€Ÿåº¦
                self._state_buffer[:, i, base_idx] = neighbor_pos[:, j]
                self._state_buffer[:, i, base_idx + 1] = neighbor_vel[:, j]
                # è§’è‰² one-hot
                self._state_buffer[:, i, base_idx + 2:base_idx + 2 + NEIGHBOR_ROLE_DIM] = neighbor_roles[j]
        
        return self._state_buffer.clone()
    
    def _scale_reward_batch(self, reward):
        """æ‰¹é‡å¥–åŠ±ç¼©æ”¾"""
        if self.use_soft_scaling:
            mid = (self.reward_max + self.reward_min) / 2
            scale = (self.reward_max - self.reward_min) / 2
            normalized = (reward - mid) / (scale + 1e-8)
            return mid + scale * torch.tanh(normalized)
        else:
            return torch.clamp(reward, self.reward_min, self.reward_max)
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        self.t += DT
        
        # ğŸ”§ æ›´æ–°é¢†å¯¼è€…ï¼ˆä½¿ç”¨æ¯ä¸ªç¯å¢ƒç‹¬ç«‹çš„åŠ¨åŠ›å­¦å‚æ•°ï¼‰
        leader_pos, leader_vel = self._leader_state_batch(self.t)
        self.positions[:, 0] = leader_pos
        self.velocities[:, 0] = leader_vel
        self.last_broadcast_pos[:, 0] = leader_pos
        self.last_broadcast_vel[:, 0] = leader_vel
        
        # è§£æåŠ¨ä½œ
        # ğŸ”§ Actor è¾“å‡ºçš„ç¬¬ 0 ç»´å·²ç»æŒ‰ V_SCALE ç¼©æ”¾è¿‡ï¼Œè¿™é‡Œä¸å†äºŒæ¬¡ç¼©æ”¾
        delta_v = action[:, :, 0]
        raw_threshold = action[:, :, 1]
        
        # é˜ˆå€¼æ˜ å°„
        normalized_threshold = raw_threshold / self.th_scale
        normalized_threshold = normalized_threshold.clamp(0.0, 1.0)
        threshold = self.threshold_min + (self.threshold_max - self.threshold_min) * normalized_threshold
        threshold = threshold.clamp(min=max(0.001, self.threshold_min), 
                                     max=min(self.threshold_max, 1.0))
        
        # æ— æ¨¡å‹åŠ¨åŠ›å­¦
        follower_vel = self.velocities[:, 1:]
        follower_pos = self.positions[:, 1:]
        
        new_vel = follower_vel + delta_v
        new_vel = torch.clamp(new_vel, -self.vel_limit, self.vel_limit)
        
        new_pos = follower_pos + new_vel * DT
        new_pos = torch.clamp(new_pos, -self.pos_limit, self.pos_limit)
        
        self.positions[:, 1:] = new_pos
        self.velocities[:, 1:] = new_vel
        
        # äº‹ä»¶è§¦å‘é€šä¿¡
        trigger_error = torch.abs(new_pos - self.last_broadcast_pos[:, 1:])
        is_triggered = trigger_error > threshold
        
        self.last_broadcast_pos[:, 1:] = torch.where(
            is_triggered, self.positions[:, 1:], self.last_broadcast_pos[:, 1:]
        )
        self.last_broadcast_vel[:, 1:] = torch.where(
            is_triggered, self.velocities[:, 1:], self.last_broadcast_vel[:, 1:]
        )
        
        # ==================== è®¡ç®—å¥–åŠ± ====================
        pos_error = torch.abs(self.positions[:, 1:] - self.positions[:, 0:1])
        vel_error = torch.abs(self.velocities[:, 1:] - self.velocities[:, 0:1])
        tracking_error = pos_error.mean(dim=1) + 0.5 * vel_error.mean(dim=1)
        
        # 1. è·Ÿè¸ªæƒ©ç½š
        tracking_penalty = -torch.tanh(tracking_error * self.tracking_penalty_scale) * self.tracking_penalty_max
        
        # 2. æ”¹è¿›å¥–åŠ±
        improvement_bonus = torch.zeros_like(tracking_error)
        valid_mask = self._prev_error_valid
        if valid_mask.any():
            improvement = self._prev_error - tracking_error
            improvement_bonus = torch.where(
                valid_mask,
                torch.clamp(improvement * self.improvement_scale, 
                           -self.improvement_clip, self.improvement_clip),
                torch.zeros_like(improvement)
            )
        
        # æ›´æ–° prev_error
        self._prev_error = tracking_error.detach().clone()
        self._prev_error_valid[:] = True
        
        # 3. é€šä¿¡æƒ©ç½š
        comm_weight = torch.exp(-tracking_error * self.comm_weight_decay)
        comm_rate = is_triggered.float().mean(dim=1)
        comm_penalty = -comm_rate * self.comm_penalty_base * comm_weight
        
        # æ€»å¥–åŠ±
        raw_reward = tracking_penalty + improvement_bonus + comm_penalty
        rewards = self._scale_reward_batch(raw_reward)
        
        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=DEVICE)
        
        infos = {
            'tracking_error': tracking_error,
            'comm_rate': comm_rate,
            'comm_weight': comm_weight,
            'leader_pos': self.positions[:, 0],
            'leader_vel': self.velocities[:, 0],
            'avg_follower_pos': self.positions[:, 1:].mean(dim=1),
            'threshold_mean': threshold.mean(),
            'tracking_penalty': tracking_penalty.mean(),
            'improvement_bonus': improvement_bonus.mean(),
            'comm_penalty': comm_penalty.mean(),
            # ğŸ”§ æ–°å¢ï¼šé¢†å¯¼è€…åŠ¨åŠ›å­¦å‚æ•°ä¿¡æ¯
            'leader_amplitude_mean': self.leader_amplitude.mean(),
            'leader_omega_mean': self.leader_omega.mean(),
        }
        
        return self._get_state_optimized(), rewards, dones, infos


class ModelFreeEnv:
    """å•ç¯å¢ƒç‰ˆæœ¬"""
    
    def __init__(self, topology):
        self.batched_env = BatchedModelFreeEnv(topology, num_envs=1)
        self.topology = topology
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
    
    @property
    def positions(self):
        return self.batched_env.positions[0]
    
    @property
    def velocities(self):
        return self.batched_env.velocities[0]
    
    @property
    def t(self):
        return self.batched_env.t[0].item()
    
    @property
    def leader_amplitude(self):
        return self.batched_env.leader_amplitude[0].item()
    
    @property
    def leader_omega(self):
        return self.batched_env.leader_omega[0].item()
    
    def get_global_state(self):
        """è·å–å…¨å±€çŠ¶æ€"""
        return self.batched_env.get_global_state()[0]
    
    def reset(self):
        state = self.batched_env.reset()
        return state[0]
    
    def step(self, action):
        action_batched = action.unsqueeze(0)
        states, rewards, dones, infos = self.batched_env.step(action_batched)
        info = {k: (v[0].item() if isinstance(v, torch.Tensor) and v.dim() > 0 else
                    v.item() if isinstance(v, torch.Tensor) else v)
                for k, v in infos.items()}
        return states[0], rewards[0].item(), dones[0].item(), info


# å…¼å®¹æ—§æ¥å£
BatchedLeaderFollowerEnv = BatchedModelFreeEnv
LeaderFollowerMASEnv = ModelFreeEnv