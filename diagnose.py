"""è¿è¡Œç¯å¢ƒè¯Šæ–­è„šæœ¬ã€‚

ç”¨äºå¿«é€Ÿç¡®è®¤ CUDA / PyTorch ç¯å¢ƒæ˜¯å¦å¯ç”¨ï¼Œå¹¶é€šè¿‡ä¸¤ç»„çŸ©é˜µä¹˜æ³•ç²—æµ‹ GPU æ€§èƒ½ã€‚
è¯¥è„šæœ¬åªåšæ‰“å°è¾“å‡ºï¼Œä¸ä¼šä¿®æ”¹ä»»ä½•è®­ç»ƒé…ç½®æˆ–æ–‡ä»¶ã€‚

ç”¨æ³•:
    python diagnose.py
"""

from __future__ import annotations

import time

import torch


def diagnose() -> None:
    """æ‰“å° CUDA çŠ¶æ€ä¸ç²—ç²’åº¦æ€§èƒ½æŒ‡æ ‡ã€‚"""

    print("=" * 60)
    print("ğŸ” æ€§èƒ½è¯Šæ–­")
    print("=" * 60)

    # 1) CUDA æ£€æŸ¥
    print("\n1ï¸âƒ£ CUDA çŠ¶æ€:")
    print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if not torch.cuda.is_available():
        print("   âŒ CUDA ä¸å¯ç”¨ï¼šè¯·æ£€æŸ¥é©±åŠ¨/å®¹å™¨æˆ– PyTorch å®‰è£…æ˜¯å¦ä¸º CUDA ç‰ˆæœ¬")
        return

    print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

    props = torch.cuda.get_device_properties(0)
    print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    print(f"   æ˜¾å­˜æ€»é‡: {props.total_memory / 1024**3:.2f} GB")

    # 2) PyTorch ç‰ˆæœ¬
    print(f"\n2ï¸âƒ£ PyTorch: {torch.__version__}")

    # 3) ç®€å• GPU æµ‹è¯•ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
    print("\n3ï¸âƒ£ GPU ç²—æµ‹ï¼ˆmatmulï¼‰:")

    # å°çŸ©é˜µ
    x_small = torch.randn(100, 100, device="cuda")
    start = time.time()
    for _ in range(1000):
        _ = torch.matmul(x_small, x_small)
    torch.cuda.synchronize()
    t_small = time.time() - start
    print(f"   å°çŸ©é˜µ (100x100, 1000 æ¬¡): {t_small:.3f}s")

    # å¤§çŸ©é˜µ
    x_large = torch.randn(2000, 2000, device="cuda")
    start = time.time()
    for _ in range(100):
        _ = torch.matmul(x_large, x_large)
    torch.cuda.synchronize()
    t_large = time.time() - start
    print(f"   å¤§çŸ©é˜µ (2000x2000, 100 æ¬¡): {t_large:.3f}s")

    # 4) æ˜¾å­˜çŠ¶æ€
    print(f"\n4ï¸âƒ£ GPU æ˜¾å­˜:")
    print(f"   å·²åˆ†é… (allocated): {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    print(f"   å·²ä¿ç•™ (reserved):   {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")

    # 5) å»ºè®®
    print("\n" + "=" * 60)
    print("ğŸ’¡ å»ºè®®:")

    if t_small > 0.5:
        print("   - å°çŸ©é˜µå¾ˆæ…¢ï¼šå¯èƒ½æœ‰å…¶ä»–è¿›ç¨‹å ç”¨ GPUï¼Œå»ºè®®ç”¨ `nvidia-smi` æ£€æŸ¥")
    if t_large > 1.0:
        print("   - å¤§çŸ©é˜µåæ…¢ï¼šå¯èƒ½å‡ºç°æ•£çƒ­/åŠŸè€—é™åˆ¶ï¼ˆthermal throttlingï¼‰")

    print("   - è®­ç»ƒæ—¶å¯ç”¨ `watch -n 1 nvidia-smi` è§‚å¯Ÿåˆ©ç”¨ç‡ä¸æ˜¾å­˜")
    print("=" * 60)


if __name__ == "__main__":
    diagnose()
