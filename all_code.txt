"""
ÈÖçÁΩÆÊñá‰ª∂ - ÈÄüÂ∫¶‰ºòÂåñÁâà
"""
import torch
import random
import numpy as np

# ==========================================
# ËÆæÂ§áÈÖçÁΩÆ
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# ÈöèÊú∫ÁßçÂ≠ê
# ==========================================
SEED = 42

def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# ==========================================
# Á≥ªÁªüÈÖçÁΩÆ
# ==========================================
NUM_FOLLOWERS = 9
NUM_AGENTS = NUM_FOLLOWERS + 1
LEADER_ID = 0
STATE_DIM = 4
HIDDEN_DIM = 128
ACTION_DIM = 2

# ==========================================
# ÁéØÂ¢ÉÈÖçÁΩÆ
# ==========================================
DT = 0.05
MAX_STEPS = 300
COMM_PENALTY = 0.1

LEADER_AMPLITUDE = 2.0
LEADER_OMEGA = 0.5
LEADER_PHASE = 0.0

POS_LIMIT = 10.0
VEL_LIMIT = 5.0

REWARD_MIN = -10.0
REWARD_MAX = 2.0
USE_SOFT_REWARD_SCALING = True

# ==========================================
# SAC Ë∂ÖÂèÇÊï∞
# ==========================================
BUFFER_SIZE = 500000
BATCH_SIZE = 512           #
GAMMA = 0.99
TAU = 0.005
LEARNING_RATE = 3e-4
ALPHA_LR = 3e-4
LOG_STD_MIN = -20
LOG_STD_MAX = 2
INIT_ALPHA = 0.2

# ==========================================
# ËÆ≠ÁªÉÈÖçÁΩÆ (ÈÄüÂ∫¶‰ºòÂåñ) ‚¨áÔ∏è ÂÖ≥ÈîÆ‰øÆÊîπ
# ==========================================
NUM_EPISODES = 400
VIS_INTERVAL = 20           # ÂáèÂ∞ëÂèØËßÜÂåñÈ¢ëÁéá
SAVE_MODEL_PATH = 'best_leader_follower_model.pth'

NUM_PARALLEL_ENVS = 32
UPDATE_FREQUENCY = 4
GRADIENT_STEPS = 4   

# Ê∑∑ÂêàÁ≤æÂ∫¶
USE_AMP = True

# ==========================================
# ÊãìÊâëÈÖçÁΩÆ
# ==========================================
NUM_PINNED = 3
TOPOLOGY_SEED = 42


def print_config():
    # ËÆ°ÁÆóÊØè episode ÁöÑÊõ¥Êñ∞Ê¨°Êï∞
    updates_per_ep = MAX_STEPS // UPDATE_FREQUENCY
    total_gradient_steps = updates_per_ep * GRADIENT_STEPS
    
    print("=" * 60)
    print("üîß Configuration (Speed Optimized)")
    print("=" * 60)
    print(f"  Device: {DEVICE}")
    print(f"  Parallel Envs: {NUM_PARALLEL_ENVS}")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Update Frequency: every {UPDATE_FREQUENCY} steps")
    print(f"  Gradient Steps: {GRADIENT_STEPS}")
    print(f"  Updates per Episode: {updates_per_ep}")
    print(f"  Total Gradient Steps per Episode: {total_gradient_steps}")
    print(f"  AMP Training: {USE_AMP}")
    print("=" * 60)"""
ÊúâÂêëÁîüÊàêÊ†ëÈÄö‰ø°ÊãìÊâë
"""
import torch
import numpy as np

try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

from config import DEVICE, NUM_PINNED, TOPOLOGY_SEED


class DirectedSpanningTreeTopology:
    r"""
    ÊúâÂêëÁîüÊàêÊ†ëÈÄö‰ø°ÊãìÊâë
    
    ÊãìÊâëÁªìÊûÑÁ§∫ÊÑè (È¢ÜÂØºËÄÖ‰∏∫Ê†πËäÇÁÇπ):
    
                    Leader (0)
                   /    |    \
                 F1    F2    F3     <- Pinned Followers
                / \     |     \
              F4  F5   F6    F7     <- Other Followers
                       / \
                     F8  F9
    """
    
    def __init__(self, num_followers, num_pinned=NUM_PINNED, seed=TOPOLOGY_SEED):
        self.num_followers = num_followers
        self.num_agents = num_followers + 1
        self.num_pinned = min(num_pinned, num_followers)
        self.leader_id = 0
        
        np.random.seed(seed)
        self._build_topology()
        
    def _build_topology(self):
        """ÊûÑÂª∫ÊúâÂêëÁîüÊàêÊ†ë"""
        edges = []
        follower_ids = list(range(1, self.num_agents))
        
        # ÈöèÊú∫ÈÄâÊã© pinned followers
        self.pinned_followers = sorted(np.random.choice(
            follower_ids, self.num_pinned, replace=False
        ).tolist())
        
        # È¢ÜÂØºËÄÖ -> pinned followers
        for f in self.pinned_followers:
            edges.append((self.leader_id, f))
        
        # ÊûÑÂª∫ÁîüÊàêÊ†ë
        unpinned = [f for f in follower_ids if f not in self.pinned_followers]
        connected = set(self.pinned_followers)
        
        for f in unpinned:
            parent = np.random.choice(list(connected))
            edges.append((parent, f))
            connected.add(f)
        
        # Ê∑ªÂä†È¢ùÂ§ñËæπ
        for f in follower_ids:
            potential_neighbors = [n for n in follower_ids if n != f]
            if potential_neighbors and np.random.random() < 0.3:
                neighbor = np.random.choice(potential_neighbors)
                if (neighbor, f) not in edges and (f, neighbor) not in edges:
                    edges.append((neighbor, f))
        
        # ËΩ¨Êç¢‰∏∫ PyTorch Ê†ºÂºè
        src, dst = zip(*edges)
        self.edge_index = torch.tensor([src, dst], dtype=torch.long).to(DEVICE)
        self._compute_degrees()
        
    def _compute_degrees(self):
        """ËÆ°ÁÆóËäÇÁÇπÂ∫¶Êï∞"""
        self.in_degree = torch.zeros(self.num_agents, device=DEVICE)
        self.out_degree = torch.zeros(self.num_agents, device=DEVICE)
        
        for i in range(self.edge_index.shape[1]):
            src, dst = self.edge_index[0, i].item(), self.edge_index[1, i].item()
            self.out_degree[src] += 1
            self.in_degree[dst] += 1
    
    def get_neighbors(self, node_id):
        """Ëé∑ÂèñËäÇÁÇπÁöÑÂÖ•ËæπÈÇªÂ±Ö"""
        mask = self.edge_index[1] == node_id
        return self.edge_index[0, mask].tolist()
    
    def has_leader_access(self, node_id):
        """Ê£ÄÊü•ËäÇÁÇπÊòØÂê¶Áõ¥Êé•ËøûÊé•È¢ÜÂØºËÄÖ"""
        return node_id in self.pinned_followers
    
    def visualize(self, save_path=None):
        """ÂèØËßÜÂåñÊãìÊâëÁªìÊûÑ"""
        if not HAS_MATPLOTLIB:
            print("matplotlib not available")
            return
            
        try:
            import networkx as nx
        except ImportError:
            print("Please install networkx: pip install networkx")
            return
        
        G = nx.DiGraph()
        G.add_nodes_from(range(self.num_agents))
        
        edges = list(zip(
            self.edge_index[0].cpu().numpy(),
            self.edge_index[1].cpu().numpy()
        ))
        G.add_edges_from(edges)
        
        pos = nx.spring_layout(G, seed=42, k=2)
        pos[0] = np.array([0.5, 1.0])
        
        plt.figure(figsize=(10, 8))
        
        nx.draw_networkx_nodes(G, pos, nodelist=[0], 
                              node_color='gold', node_size=800, label='Leader')
        nx.draw_networkx_nodes(G, pos, nodelist=self.pinned_followers,
                              node_color='lightgreen', node_size=500, label='Pinned')
        other_nodes = [n for n in range(1, self.num_agents) if n not in self.pinned_followers]
        nx.draw_networkx_nodes(G, pos, nodelist=other_nodes,
                              node_color='lightblue', node_size=400, label='Others')
        
        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=20, alpha=0.7)
        
        labels = {0: 'L'}
        labels.update({i: f'F{i}' for i in range(1, self.num_agents)})
        nx.draw_networkx_labels(G, pos, labels, font_size=10)
        
        plt.title('Directed Spanning Tree Topology', fontsize=12)
        plt.legend(loc='upper left')
        plt.axis('off')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"\nüìä Topology Statistics:")
        print(f"   Nodes: {self.num_agents}, Edges: {self.edge_index.shape[1]}")
        print(f"   Pinned Followers: {self.pinned_followers}")"""
ÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫ - GPU ‰ºòÂåñÁâà
"""
import torch
from config import DEVICE, BUFFER_SIZE, STATE_DIM, ACTION_DIM, NUM_AGENTS


class OptimizedReplayBuffer:
    """
    GPU È¢ÑÂàÜÈÖçÁöÑÈ´òÊïàÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫
    
    ÊîØÊåÅÊâπÈáèÂ≠òÂÇ®ÂíåÈááÊ†∑
    """
    
    def __init__(self, capacity=BUFFER_SIZE, num_agents=NUM_AGENTS, 
                 state_dim=STATE_DIM, action_dim=ACTION_DIM):
        self.capacity = capacity
        self.num_agents = num_agents
        self.num_followers = num_agents - 1
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        self.ptr = 0
        self.size = 0
        
        # È¢ÑÂàÜÈÖç GPU ÂÜÖÂ≠ò
        self.states = torch.zeros(capacity, num_agents, state_dim, device=DEVICE)
        self.actions = torch.zeros(capacity, self.num_followers, action_dim, device=DEVICE)
        self.rewards = torch.zeros(capacity, device=DEVICE)
        self.next_states = torch.zeros(capacity, num_agents, state_dim, device=DEVICE)
        self.dones = torch.zeros(capacity, device=DEVICE)
    
    def push(self, state, action, reward, next_state, done):
        """Â≠òÂÇ®ÂçïÊù°ÁªèÈ™å"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def push_batch(self, states, actions, rewards, next_states, dones):
        """
        ÊâπÈáèÂ≠òÂÇ®ÁªèÈ™å
        
        Args:
            states: (batch, num_agents, state_dim)
            actions: (batch, num_followers, action_dim)
            rewards: (batch,)
            next_states: (batch, num_agents, state_dim)
            dones: (batch,)
        """
        batch_size = states.shape[0]
        
        # ËÆ°ÁÆóÂ≠òÂÇ®‰ΩçÁΩÆ
        if self.ptr + batch_size <= self.capacity:
            # ‰∏çÈúÄË¶ÅÁéØÁªï
            idx = slice(self.ptr, self.ptr + batch_size)
            self.states[idx] = states
            self.actions[idx] = actions
            self.rewards[idx] = rewards
            self.next_states[idx] = next_states
            self.dones[idx] = dones.float()
        else:
            # ÈúÄË¶ÅÁéØÁªïÂ§ÑÁêÜ
            first_part = self.capacity - self.ptr
            second_part = batch_size - first_part
            
            self.states[self.ptr:] = states[:first_part]
            self.states[:second_part] = states[first_part:]
            
            self.actions[self.ptr:] = actions[:first_part]
            self.actions[:second_part] = actions[first_part:]
            
            self.rewards[self.ptr:] = rewards[:first_part]
            self.rewards[:second_part] = rewards[first_part:]
            
            self.next_states[self.ptr:] = next_states[:first_part]
            self.next_states[:second_part] = next_states[first_part:]
            
            self.dones[self.ptr:] = dones[:first_part].float()
            self.dones[:second_part] = dones[first_part:].float()
        
        self.ptr = (self.ptr + batch_size) % self.capacity
        self.size = min(self.size + batch_size, self.capacity)
    
    def sample(self, batch_size):
        """ÈöèÊú∫ÈááÊ†∑"""
        indices = torch.randint(0, self.size, (batch_size,), device=DEVICE)
        
        return (
            self.states[indices],
            self.actions[indices],
            self.rewards[indices],
            self.next_states[indices],
            self.dones[indices]
        )
    
    def __len__(self):
        return self.size
    
    def is_ready(self, batch_size):
        return self.size >= batch_size"""
Á•ûÁªèÁΩëÁªúÊ®°Âûã - GAT ÁºñÁ†ÅÂô®„ÄÅActor„ÄÅCritic
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

try:
    from torch_geometric.nn import GATConv
except ImportError:
    raise ImportError(
        "torch_geometric Êú™ÂÆâË£ÖÔºåËØ∑ËøêË°å:\n"
        "pip install torch-geometric\n"
        "ÊàñÂèÇËÄÉ: https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html"
    )

from config import (
    STATE_DIM, HIDDEN_DIM, ACTION_DIM,
    LOG_STD_MIN, LOG_STD_MAX
)


class TopologyAwareGATEncoder(nn.Module):
    """ÊãìÊâëÊÑüÁü•ÁöÑÂõæÊ≥®ÊÑèÂäõÁºñÁ†ÅÂô®"""
    
    def __init__(self, in_dim=STATE_DIM, hidden_dim=HIDDEN_DIM, num_heads=4):
        super(TopologyAwareGATEncoder, self).__init__()
        
        self.gat1 = GATConv(in_dim, hidden_dim, heads=num_heads, concat=True, 
                           add_self_loops=True, dropout=0.1)
        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, 
                           concat=True, add_self_loops=True, dropout=0.1)
        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, 
                           concat=False, add_self_loops=True)
        
        self.norm1 = nn.LayerNorm(hidden_dim * num_heads)
        self.norm2 = nn.LayerNorm(hidden_dim * num_heads)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        self.role_embedding = nn.Embedding(2, hidden_dim)
        
        self.output_proj = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    
    def forward(self, x, edge_index, role_ids):
        h = F.elu(self.gat1(x, edge_index))
        h = self.norm1(h)
        
        h = F.elu(self.gat2(h, edge_index))
        h = self.norm2(h)
        
        h = self.gat3(h, edge_index)
        h = self.norm3(h)
        
        role_emb = self.role_embedding(role_ids)
        h = self.output_proj(torch.cat([h, role_emb], dim=-1))
        
        return h


class GaussianActor(nn.Module):
    """È´òÊñØÁ≠ñÁï• Actor ÁΩëÁªú"""
    
    def __init__(self, state_dim=STATE_DIM, hidden_dim=HIDDEN_DIM, num_heads=4):
        super(GaussianActor, self).__init__()
        
        self.encoder = TopologyAwareGATEncoder(state_dim, hidden_dim, num_heads)
        
        self.u_mean = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        self.u_log_std = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self.th_mean = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        self.th_log_std = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self.u_scale = 5.0
        self.th_scale = 0.8
        
        # Êï∞ÂÄºÁ®≥ÂÆöÊÄßÂ∏∏Êï∞
        self._eps = 1e-6
    
    def forward(self, x, edge_index, role_ids, deterministic=False):
        feat = self.encoder(x, edge_index, role_ids)
        
        follower_mask = role_ids == 1
        follower_feat = feat[follower_mask]
        
        u_mean = self.u_mean(follower_feat)
        u_log_std = torch.clamp(self.u_log_std(follower_feat), LOG_STD_MIN, LOG_STD_MAX)
        u_std = torch.exp(u_log_std)
        
        th_mean = self.th_mean(follower_feat)
        th_log_std = torch.clamp(self.th_log_std(follower_feat), LOG_STD_MIN, LOG_STD_MAX)
        th_std = torch.exp(th_log_std)
        
        if deterministic:
            u = torch.tanh(u_mean) * self.u_scale
            th = torch.sigmoid(th_mean) * self.th_scale
            log_prob = None
        else:
            u_dist = Normal(u_mean, u_std)
            th_dist = Normal(th_mean, th_std)
            
            u_sample = u_dist.rsample()
            th_sample = th_dist.rsample()
            
            # Â∫îÁî®ÂèòÊç¢
            u_tanh = torch.tanh(u_sample)
            u = u_tanh * self.u_scale
            
            th_sigmoid = torch.sigmoid(th_sample)
            th = th_sigmoid * self.th_scale
            
            # Êï∞ÂÄºÁ®≥ÂÆöÁöÑ log_prob ËÆ°ÁÆó
            # ÂØπ‰∫é tanh ÂèòÊç¢: log|det(df/dx)| = log(1 - tanh^2(x))
            log_prob_u = u_dist.log_prob(u_sample) - torch.log(
                torch.clamp(1.0 - u_tanh.pow(2), min=self._eps, max=1.0)
            )
            
            # ÂØπ‰∫é sigmoid ÂèòÊç¢: log|det(df/dx)| = log(sigmoid(x) * (1 - sigmoid(x)))
            log_prob_th = th_dist.log_prob(th_sample) - torch.log(
                torch.clamp(th_sigmoid * (1.0 - th_sigmoid), min=self._eps, max=0.25)
            )
            
            log_prob = (log_prob_u + log_prob_th).sum(dim=-1, keepdim=True)
        
        action = torch.cat([u, th], dim=-1)
        return action, log_prob, follower_mask


class SoftQNetwork(nn.Module):
    """Soft Q ÁΩëÁªú"""
    
    def __init__(self, state_dim=STATE_DIM, hidden_dim=HIDDEN_DIM, 
                 action_dim=ACTION_DIM, num_heads=4):
        super(SoftQNetwork, self).__init__()
        
        self.encoder = TopologyAwareGATEncoder(state_dim, hidden_dim, num_heads)
        
        self.q_net = nn.Sequential(
            nn.Linear(hidden_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, x, edge_index, role_ids, action):
        feat = self.encoder(x, edge_index, role_ids)
        
        follower_mask = role_ids == 1
        follower_feat = feat[follower_mask]
        
        q_input = torch.cat([follower_feat, action], dim=-1)
        q_value = self.q_net(q_input)
        
        return q_value"""
È¢ÜÂØºËÄÖ-Ë∑üÈöèËÄÖÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁéØÂ¢É - Â¢ûÂº∫Áâà
"""
import torch
import math

from config import (
    DEVICE, STATE_DIM, DT, COMM_PENALTY,
    LEADER_AMPLITUDE, LEADER_OMEGA, LEADER_PHASE,
    POS_LIMIT, VEL_LIMIT,
    REWARD_MIN, REWARD_MAX, USE_SOFT_REWARD_SCALING
)


class BatchedLeaderFollowerEnv:
    """ÂÆåÂÖ®ÂêëÈáèÂåñÁöÑÊâπÈáèÁéØÂ¢É - Â¢ûÂº∫Áâà"""
    
    def __init__(self, topology, num_envs=64):
        self.topology = topology
        self.num_envs = num_envs
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
        self.leader_id = topology.leader_id
        
        self.leader_amplitude = LEADER_AMPLITUDE
        self.leader_omega = LEADER_OMEGA
        self.leader_phase = LEADER_PHASE
        
        self.pos_limit = POS_LIMIT
        self.vel_limit = VEL_LIMIT
        self.reward_min = REWARD_MIN
        self.reward_max = REWARD_MAX
        self.use_soft_scaling = USE_SOFT_REWARD_SCALING
        
        # üîß Â¢ûÂº∫ÊéßÂà∂Âô®Â¢ûÁõä
        self.base_pos_gain = 5.0   # ‰ΩçÁΩÆÂèçÈ¶àÂ¢ûÁõä (Â¢ûÂ§ß)
        self.base_vel_gain = 2.5   # ÈÄüÂ∫¶ÂèçÈ¶àÂ¢ûÁõä (Â¢ûÂ§ß)
        
        self.role_ids = torch.zeros(self.num_agents, dtype=torch.long, device=DEVICE)
        self.role_ids[1:] = 1
        
        self._precompute_neighbor_info()
        
        # È¢ÑÂàÜÈÖçÁä∂ÊÄÅÂº†Èáè
        self.positions = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.velocities = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.last_broadcast_pos = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.last_broadcast_vel = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.t = torch.zeros(num_envs, device=DEVICE)
        
        # ËØØÂ∑ÆËÆ∞ÂΩï
        self._prev_error = None
        
        self.reset()
    
    def _precompute_neighbor_info(self):
        """È¢ÑËÆ°ÁÆóÈÇªÂ±ÖËÅöÂêàÁü©Èòµ"""
        self.adj_matrix = torch.zeros(self.num_agents, self.num_agents, device=DEVICE)
        edge_index = self.topology.edge_index
        
        for i in range(edge_index.shape[1]):
            src, dst = edge_index[0, i].item(), edge_index[1, i].item()
            self.adj_matrix[dst, src] = 1.0
        
        in_degree = self.adj_matrix.sum(dim=1)
        self.degree_matrix = torch.diag(in_degree)
        self.laplacian = self.degree_matrix - self.adj_matrix
        
        in_degree_safe = in_degree.clamp(min=1.0)
        self.norm_adj_matrix = self.adj_matrix / in_degree_safe.unsqueeze(1)
        
        # pinning Â¢ûÁõä
        self.pinning_gains = torch.zeros(self.num_agents, device=DEVICE)
        for f in self.topology.pinned_followers:
            self.pinning_gains[f] = 2.0  # üîß Â¢ûÂº∫ pinning Â¢ûÁõä
    
    def _leader_state_batch(self, t):
        """ÊâπÈáèËÆ°ÁÆóÈ¢ÜÂØºËÄÖÁä∂ÊÄÅ"""
        pos = self.leader_amplitude * torch.sin(self.leader_omega * t + self.leader_phase)
        vel = self.leader_amplitude * self.leader_omega * torch.cos(self.leader_omega * t + self.leader_phase)
        return pos, vel
    
    def reset(self, env_ids=None):
        """ÈáçÁΩÆÁéØÂ¢É"""
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        
        num_reset = len(env_ids) if isinstance(env_ids, torch.Tensor) else self.num_envs
        
        self.t[env_ids] = 0.0
        
        leader_pos, leader_vel = self._leader_state_batch(self.t[env_ids])
        
        self.positions[env_ids, 0] = leader_pos
        self.velocities[env_ids, 0] = leader_vel
        
        # üîß ÂàùÂßã‰ΩçÁΩÆÊõ¥Êé•ËøëÈ¢ÜÂØºËÄÖ
        self.positions[env_ids, 1:] = leader_pos.unsqueeze(1) + torch.randn(num_reset, self.num_followers, device=DEVICE) * 0.3
        self.velocities[env_ids, 1:] = leader_vel.unsqueeze(1) + torch.randn(num_reset, self.num_followers, device=DEVICE) * 0.1
        
        self.last_broadcast_pos[env_ids] = self.positions[env_ids].clone()
        self.last_broadcast_vel[env_ids] = self.velocities[env_ids].clone()
        
        self._prev_error = None
        
        return self._get_state()
    
    def _get_state(self):
        """ÊûÑÂª∫ËßÇÊµãÁä∂ÊÄÅ"""
        state = torch.zeros(self.num_envs, self.num_agents, STATE_DIM, device=DEVICE)
        
        neighbor_avg_pos = torch.matmul(self.last_broadcast_pos, self.norm_adj_matrix.T)
        neighbor_avg_vel = torch.matmul(self.last_broadcast_vel, self.norm_adj_matrix.T)
        
        pos_error = self.positions - neighbor_avg_pos
        vel_error = self.velocities - neighbor_avg_vel
        
        # üîß ÂΩí‰∏ÄÂåñÁä∂ÊÄÅÔºåÂ∏ÆÂä©ÁΩëÁªúÂ≠¶‰π†
        state[:, :, 0] = pos_error / (self.pos_limit + 1e-6)
        state[:, :, 1] = vel_error / (self.vel_limit + 1e-6)
        state[:, :, 2] = self.positions / (self.pos_limit + 1e-6)
        state[:, :, 3] = self.velocities / (self.vel_limit + 1e-6)
        
        return state
    
    def _compute_base_control(self):
        """ËÆ°ÁÆóÂü∫Á°Ä‰∏ÄËá¥ÊÄßÊéßÂà∂"""
        follower_pos = self.last_broadcast_pos[:, 1:]
        follower_vel = self.last_broadcast_vel[:, 1:]
        
        leader_pos = self.last_broadcast_pos[:, 0:1]
        leader_vel = self.last_broadcast_vel[:, 0:1]
        
        follower_adj = self.adj_matrix[1:, 1:]
        follower_degree = follower_adj.sum(dim=1, keepdim=True).clamp(min=1.0)
        
        neighbor_pos_sum = torch.matmul(follower_pos, follower_adj.T)
        neighbor_vel_sum = torch.matmul(follower_vel, follower_adj.T)
        
        pos_consensus_error = follower_pos * follower_degree.T - neighbor_pos_sum
        vel_consensus_error = follower_vel * follower_degree.T - neighbor_vel_sum
        
        pinning_gains_followers = self.pinning_gains[1:]
        pos_pinning_error = (follower_pos - leader_pos) * pinning_gains_followers
        vel_pinning_error = (follower_vel - leader_vel) * pinning_gains_followers
        
        base_control = (
            -self.base_pos_gain * (pos_consensus_error + pos_pinning_error)
            -self.base_vel_gain * (vel_consensus_error + vel_pinning_error)
        )
        
        return base_control
    
    def _scale_reward_batch(self, reward):
        """ÊâπÈáèÂ•ñÂä±Áº©Êîæ"""
        if self.use_soft_scaling:
            mid = (self.reward_max + self.reward_min) / 2
            scale = (self.reward_max - self.reward_min) / 2
            normalized = (reward - mid) / (scale + 1e-8)
            return mid + scale * torch.tanh(normalized)
        else:
            return torch.clamp(reward, self.reward_min, self.reward_max)
    
    def step(self, action):
        """ÊâπÈáèÊâßË°å‰∏ÄÊ≠•"""
        self.t += DT
        
        # Êõ¥Êñ∞È¢ÜÂØºËÄÖ
        leader_pos, leader_vel = self._leader_state_batch(self.t)
        self.positions[:, 0] = leader_pos
        self.velocities[:, 0] = leader_vel
        
        # üîß Ëß£ÊûêÂä®‰Ωú - Â¢ûÂ§ßÊéßÂà∂Ë∞ÉÊï¥ËåÉÂõ¥
        delta_u = action[:, :, 0] * 2.0  # ÊîæÂ§ß delta_u ÁöÑÂΩ±Âìç
        threshold = action[:, :, 1].clamp(min=0.01, max=0.8)
        
        # ËÆ°ÁÆóÊÄªÊéßÂà∂
        base_u = self._compute_base_control()
        total_u = base_u + delta_u
        
        # üîß ÈôêÂà∂ÊéßÂà∂ËæìÂÖ•
        total_u = torch.clamp(total_u, -20.0, 20.0)
        
        # Ë∑üÈöèËÄÖÂä®ÂäõÂ≠¶
        follower_pos = self.positions[:, 1:]
        follower_vel = self.velocities[:, 1:]
        
        # üîß ÂáèÂº±ÈùûÁ∫øÊÄßÂπ≤Êâ∞
        nonlinear_term = 0.2 * torch.sin(follower_pos) - 0.1 * follower_vel
        
        acc = total_u + nonlinear_term
        
        new_vel = torch.clamp(follower_vel + acc * DT, -self.vel_limit, self.vel_limit)
        new_pos = torch.clamp(follower_pos + new_vel * DT, -self.pos_limit, self.pos_limit)
        
        self.positions[:, 1:] = new_pos
        self.velocities[:, 1:] = new_vel
        
        # ‰∫ã‰ª∂Ëß¶ÂèëÈÄö‰ø°
        trigger_error = torch.abs(new_pos - self.last_broadcast_pos[:, 1:])
        is_triggered = trigger_error > threshold
        
        self.last_broadcast_pos[:, 1:] = torch.where(
            is_triggered, self.positions[:, 1:], self.last_broadcast_pos[:, 1:]
        )
        self.last_broadcast_vel[:, 1:] = torch.where(
            is_triggered, self.velocities[:, 1:], self.last_broadcast_vel[:, 1:]
        )
        self.last_broadcast_pos[:, 0] = self.positions[:, 0]
        self.last_broadcast_vel[:, 0] = self.velocities[:, 0]
        
        # üîß ÊîπËøõÂ•ñÂä±ËÆ°ÁÆó
        pos_error = torch.abs(self.positions[:, 1:] - self.positions[:, 0:1])
        vel_error = torch.abs(self.velocities[:, 1:] - self.velocities[:, 0:1])
        
        # ‰ΩçÁΩÆËØØÂ∑ÆÂíåÈÄüÂ∫¶ËØØÂ∑Æ
        tracking_error = (pos_error.mean(dim=1) + 0.5 * vel_error.mean(dim=1))
        
        # üîß Êõ¥Â•ΩÁöÑÂ•ñÂä±Â°ëÂΩ¢
        # ‰ΩøÁî®ÊåáÊï∞Ë°∞ÂáèÂ•ñÂä±ÔºåËØØÂ∑ÆÂ∞èÊó∂Â•ñÂä±È´ò
        tracking_reward = torch.exp(-tracking_error) * 2.0 - 1.0  # ËåÉÂõ¥ [-1, 1]
        
        # Á®≥ÂÆöÊÄßÂ•ñÂä±
        stability_bonus = torch.zeros_like(tracking_error)
        if self._prev_error is not None:
            improvement = self._prev_error - tracking_error
            stability_bonus = torch.clamp(improvement * 2.0, -0.5, 0.5)
        self._prev_error = tracking_error.detach().clone()
        
        # ÈÄö‰ø°ÊÉ©ÁΩö
        comm_rate = is_triggered.float().mean(dim=1)
        comm_penalty = comm_rate * COMM_PENALTY
        
        # üîß ÊÄªÂ•ñÂä±
        raw_reward = tracking_reward + stability_bonus - comm_penalty
        rewards = self._scale_reward_batch(raw_reward)
        
        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=DEVICE)
        
        infos = {
            'tracking_error': tracking_error,
            'comm_rate': comm_rate,
            'leader_pos': self.positions[:, 0],
            'leader_vel': self.velocities[:, 0],
            'avg_follower_pos': self.positions[:, 1:].mean(dim=1),
            'base_control_norm': base_u.abs().mean(),
            'delta_u_norm': delta_u.abs().mean(),
        }
        
        return self._get_state(), rewards, dones, infos


class LeaderFollowerMASEnv:
    """ÂçïÁéØÂ¢ÉÁâàÊú¨"""
    
    def __init__(self, topology):
        self.batched_env = BatchedLeaderFollowerEnv(topology, num_envs=1)
        self.topology = topology
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
        self.role_ids = self.batched_env.role_ids
    
    @property
    def positions(self):
        return self.batched_env.positions[0]
    
    @property
    def velocities(self):
        return self.batched_env.velocities[0]
    
    @property
    def t(self):
        return self.batched_env.t[0].item()
    
    def reset(self):
        state = self.batched_env.reset()
        return state[0]
    
    def step(self, action):
        action_batched = action.unsqueeze(0)
        states, rewards, dones, infos = self.batched_env.step(action_batched)
        info = {k: (v[0].item() if isinstance(v, torch.Tensor) and v.dim() > 0 else v) 
                for k, v in infos.items()}
        return states[0], rewards[0].item(), dones[0].item(), info"""
SAC Êô∫ËÉΩ‰Ωì - ‰ºòÂåñÁâà (‰øÆÂ§ç PyG ÂÖºÂÆπÊÄß)
"""
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

from config import (
    DEVICE, STATE_DIM, HIDDEN_DIM, ACTION_DIM, NUM_AGENTS,
    LEARNING_RATE, ALPHA_LR, GAMMA, TAU, BATCH_SIZE,
    INIT_ALPHA, GRADIENT_STEPS
)
from buffer import OptimizedReplayBuffer
from networks import GaussianActor, SoftQNetwork


class SACAgent:
    """Soft Actor-Critic Êô∫ËÉΩ‰Ωì (PyG ÂÖºÂÆπÁâà)"""
    
    def __init__(self, topology, auto_entropy=True, use_amp=True):
        self.topology = topology
        self.num_followers = topology.num_followers
        self.num_agents = topology.num_agents
        self.auto_entropy = auto_entropy
        self.use_amp = use_amp and torch.cuda.is_available()
        
        # ÁΩëÁªúÂàùÂßãÂåñ
        self.actor = GaussianActor(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q1 = SoftQNetwork(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2 = SoftQNetwork(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q1_target = SoftQNetwork(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        self.q2_target = SoftQNetwork(STATE_DIM, HIDDEN_DIM).to(DEVICE)
        
        self.q1_target.load_state_dict(self.q1.state_dict())
        self.q2_target.load_state_dict(self.q2.state_dict())
        
        # ÂÜªÁªìÁõÆÊ†áÁΩëÁªú
        for param in self.q1_target.parameters():
            param.requires_grad = False
        for param in self.q2_target.parameters():
            param.requires_grad = False
        
        # Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
        if self.use_amp:
            from torch.cuda.amp import GradScaler
            self.scaler = GradScaler()
            print("üöÄ AMP (Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ) Â∑≤ÂêØÁî®")
        else:
            self.scaler = None
        
        # ÁÜµÁ≥ªÊï∞
        self.target_entropy = -float(ACTION_DIM)
        self.log_alpha = torch.tensor(np.log(INIT_ALPHA), requires_grad=True, device=DEVICE)
        self.alpha = self.log_alpha.exp().item()
        
        # ‰ºòÂåñÂô®
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE)
        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=LEARNING_RATE)
        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=LEARNING_RATE)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=ALPHA_LR)
        
        # ÁªèÈ™åÂõûÊîæ
        self.buffer = OptimizedReplayBuffer(num_agents=NUM_AGENTS)
        
        # È¢ÑËÆ°ÁÆó
        self.role_ids = torch.zeros(self.num_agents, dtype=torch.long, device=DEVICE)
        self.role_ids[1:] = 1
        
        # ÁºìÂ≠ò
        self._edge_index_cache = {}
        self._role_ids_cache = {}
        
        self.last_losses = {'q1': 0, 'q2': 0, 'actor': 0, 'alpha': INIT_ALPHA}
        self.update_count = 0
    
    def _get_batch_graph_data(self, batch_size):
        """Ëé∑ÂèñÊâπÈáèÂõæÊï∞ÊçÆ (ÁºìÂ≠ò)"""
        if batch_size not in self._edge_index_cache:
            num_nodes = self.num_agents
            edge_indices = [self.topology.edge_index + i * num_nodes for i in range(batch_size)]
            self._edge_index_cache[batch_size] = torch.cat(edge_indices, dim=1)
            self._role_ids_cache[batch_size] = self.role_ids.repeat(batch_size)
        return self._edge_index_cache[batch_size], self._role_ids_cache[batch_size]
    
    @torch.no_grad()
    def select_action(self, state, deterministic=False):
        """ÈÄâÊã©Âä®‰Ωú"""
        is_batched = state.dim() == 3
        
        if is_batched:
            batch_size = state.shape[0]
            flat_state = state.view(-1, STATE_DIM)
            batch_edge_index, batch_role_ids = self._get_batch_graph_data(batch_size)
            
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    action, _, _ = self.actor(
                        flat_state, batch_edge_index, batch_role_ids, deterministic=deterministic
                    )
            else:
                action, _, _ = self.actor(
                    flat_state, batch_edge_index, batch_role_ids, deterministic=deterministic
                )
            action = action.view(batch_size, self.num_followers, ACTION_DIM)
        else:
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    action, _, _ = self.actor(
                        state, self.topology.edge_index, self.role_ids, deterministic=deterministic
                    )
            else:
                action, _, _ = self.actor(
                    state, self.topology.edge_index, self.role_ids, deterministic=deterministic
                )
        
        return action.float()
    
    def store_transitions_batch(self, states, actions, rewards, next_states, dones):
        """ÊâπÈáèÂ≠òÂÇ®"""
        self.buffer.push_batch(states, actions, rewards, next_states, dones)
    
    def update(self, batch_size=BATCH_SIZE, gradient_steps=GRADIENT_STEPS):
        """Êõ¥Êñ∞ÁΩëÁªú"""
        if not self.buffer.is_ready(batch_size):
            return {}
        
        total_q1_loss = 0
        total_q2_loss = 0
        total_actor_loss = 0
        
        for _ in range(gradient_steps):
            self.update_count += 1
            
            states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)
            
            flat_states = states.view(-1, STATE_DIM)
            flat_next_states = next_states.view(-1, STATE_DIM)
            flat_actions = actions.view(-1, ACTION_DIM)
            
            batch_edge_index, batch_role_ids = self._get_batch_graph_data(batch_size)
            
            # ========== Critic Êõ¥Êñ∞ ==========
            with torch.no_grad():
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        next_actions, next_log_probs, _ = self.actor(
                            flat_next_states, batch_edge_index, batch_role_ids
                        )
                        q1_next = self.q1_target(flat_next_states, batch_edge_index, batch_role_ids, next_actions)
                        q2_next = self.q2_target(flat_next_states, batch_edge_index, batch_role_ids, next_actions)
                else:
                    next_actions, next_log_probs, _ = self.actor(
                        flat_next_states, batch_edge_index, batch_role_ids
                    )
                    q1_next = self.q1_target(flat_next_states, batch_edge_index, batch_role_ids, next_actions)
                    q2_next = self.q2_target(flat_next_states, batch_edge_index, batch_role_ids, next_actions)
                
                q_next = torch.min(q1_next, q2_next)
                q_next = q_next.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                next_log_probs = next_log_probs.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                
                target_q = rewards.unsqueeze(1) + GAMMA * (1 - dones.unsqueeze(1)) * (q_next - self.alpha * next_log_probs)
                target_q = target_q.float()
            
            # Q1 Êõ¥Êñ∞
            self.q1_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    q1_curr = self.q1(flat_states, batch_edge_index, batch_role_ids, flat_actions)
                    q1_curr = q1_curr.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                    q1_loss = F.mse_loss(q1_curr.float(), target_q)
                self.scaler.scale(q1_loss).backward()
                self.scaler.unscale_(self.q1_optimizer)
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                self.scaler.step(self.q1_optimizer)
            else:
                q1_curr = self.q1(flat_states, batch_edge_index, batch_role_ids, flat_actions)
                q1_curr = q1_curr.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                q1_loss = F.mse_loss(q1_curr, target_q)
                q1_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)
                self.q1_optimizer.step()
            
            # Q2 Êõ¥Êñ∞
            self.q2_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    q2_curr = self.q2(flat_states, batch_edge_index, batch_role_ids, flat_actions)
                    q2_curr = q2_curr.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                    q2_loss = F.mse_loss(q2_curr.float(), target_q)
                self.scaler.scale(q2_loss).backward()
                self.scaler.unscale_(self.q2_optimizer)
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)
                self.scaler.step(self.q2_optimizer)
            else:
                q2_curr = self.q2(flat_states, batch_edge_index, batch_role_ids, flat_actions)
                q2_curr = q2_curr.view(batch_size, self.num_followers).mean(dim=1, keepdim=True)
                q2_loss = F.mse_loss(q2_curr, target_q)
                q2_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)
                self.q2_optimizer.step()
            
            # ========== Actor Êõ¥Êñ∞ ==========
            self.actor_optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    new_actions, log_probs, _ = self.actor(flat_states, batch_edge_index, batch_role_ids)
                    q1_new = self.q1(flat_states, batch_edge_index, batch_role_ids, new_actions)
                    q2_new = self.q2(flat_states, batch_edge_index, batch_role_ids, new_actions)
                    q_new = torch.min(q1_new, q2_new)
                    actor_loss = (self.alpha * log_probs - q_new).mean()
                self.scaler.scale(actor_loss).backward()
                self.scaler.unscale_(self.actor_optimizer)
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                self.scaler.step(self.actor_optimizer)
            else:
                new_actions, log_probs, _ = self.actor(flat_states, batch_edge_index, batch_role_ids)
                q1_new = self.q1(flat_states, batch_edge_index, batch_role_ids, new_actions)
                q2_new = self.q2(flat_states, batch_edge_index, batch_role_ids, new_actions)
                q_new = torch.min(q1_new, q2_new)
                actor_loss = (self.alpha * log_probs - q_new).mean()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
                self.actor_optimizer.step()
            
            # ========== Alpha Êõ¥Êñ∞ ==========
            if self.auto_entropy:
                self.alpha_optimizer.zero_grad(set_to_none=True)
                alpha_loss = -(self.log_alpha * (log_probs.detach() + self.target_entropy)).mean()
                alpha_loss.backward()
                self.alpha_optimizer.step()
                self.alpha = self.log_alpha.exp().item()
            
            # ========== ËΩØÊõ¥Êñ∞ ==========
            self._soft_update(self.q1, self.q1_target)
            self._soft_update(self.q2, self.q2_target)
            
            # Êõ¥Êñ∞ scaler
            if self.use_amp:
                self.scaler.update()
            
            total_q1_loss += q1_loss.item()
            total_q2_loss += q2_loss.item()
            total_actor_loss += actor_loss.item()
        
        self.last_losses = {
            'q1': total_q1_loss / gradient_steps,
            'q2': total_q2_loss / gradient_steps,
            'actor': total_actor_loss / gradient_steps,
            'alpha': self.alpha
        }
        
        return self.last_losses
    
    @torch.no_grad()
    def _soft_update(self, source, target):
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.lerp_(param.data, TAU)
    
    def save(self, path):
        torch.save({
            'actor': self.actor.state_dict(),
            'q1': self.q1.state_dict(),
            'q2': self.q2.state_dict(),
            'q1_target': self.q1_target.state_dict(),
            'q2_target': self.q2_target.state_dict(),
            'log_alpha': self.log_alpha,
            'update_count': self.update_count,
        }, path)
        print(f"‚úÖ Model saved to {path}")
    
    def load(self, path):
        checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)
        self.actor.load_state_dict(checkpoint['actor'])
        self.q1.load_state_dict(checkpoint['q1'])
        self.q2.load_state_dict(checkpoint['q2'])
        self.q1_target.load_state_dict(checkpoint['q1_target'])
        self.q2_target.load_state_dict(checkpoint['q2_target'])
        if 'log_alpha' in checkpoint:
            self.log_alpha = checkpoint['log_alpha']
            self.alpha = self.log_alpha.exp().item()
        print(f"‚úÖ Model loaded from {path}")"""
Â∑•ÂÖ∑ÂáΩÊï∞
"""
import torch
import numpy as np

try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

from config import DEVICE, MAX_STEPS


@torch.no_grad()
def collect_trajectory(agent, env, max_steps=MAX_STEPS):
    """Êî∂ÈõÜËΩ®ËøπÁî®‰∫éÂèØËßÜÂåñ"""
    state = env.reset()
    
    times = [0]
    leader_pos = [env.positions[0].item()]
    leader_vel = [env.velocities[0].item()]
    follower_pos = [env.positions[1:].cpu().numpy()]
    follower_vel = [env.velocities[1:].cpu().numpy()]
    
    for step in range(max_steps):
        action = agent.select_action(state, deterministic=True)
        state, _, _, _ = env.step(action)
        
        times.append(env.t)
        leader_pos.append(env.positions[0].item())
        leader_vel.append(env.velocities[0].item())
        follower_pos.append(env.positions[1:].cpu().numpy())
        follower_vel.append(env.velocities[1:].cpu().numpy())
    
    return {
        'times': np.array(times),
        'leader_pos': np.array(leader_pos),
        'leader_vel': np.array(leader_vel),
        'follower_pos': np.array(follower_pos),
        'follower_vel': np.array(follower_vel)
    }


@torch.no_grad()
def evaluate_agent(agent, env, num_episodes=5):
    """ËØÑ‰º∞Êô∫ËÉΩ‰ΩìÊÄßËÉΩ"""
    results = {
        'rewards': [],
        'tracking_errors': [],
        'comm_rates': []
    }
    
    for _ in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_tracking_err = 0
        episode_comm = 0
        
        for step in range(MAX_STEPS):
            action = agent.select_action(state, deterministic=True)
            state, reward, _, info = env.step(action)
            
            episode_reward += reward
            episode_tracking_err += info['tracking_error']
            episode_comm += info['comm_rate']
        
        results['rewards'].append(episode_reward)
        results['tracking_errors'].append(episode_tracking_err / MAX_STEPS)
        results['comm_rates'].append(episode_comm / MAX_STEPS)
    
    return {
        'mean_reward': np.mean(results['rewards']),
        'std_reward': np.std(results['rewards']),
        'mean_tracking_error': np.mean(results['tracking_errors']),
        'mean_comm_rate': np.mean(results['comm_rates'])
    }


def plot_evaluation(agent, topology, num_tests=3, save_path=None):
    """ÁªòÂà∂ËØÑ‰º∞ÁªìÊûú"""
    if not HAS_MATPLOTLIB:
        print("matplotlib not available")
        return
    
    from environment import LeaderFollowerMASEnv
    
    env = LeaderFollowerMASEnv(topology)
    
    fig, axes = plt.subplots(num_tests, 2, figsize=(14, 4 * num_tests))
    if num_tests == 1:
        axes = axes.reshape(1, -1)
    
    results = []
    
    for test_idx in range(num_tests):
        traj = collect_trajectory(agent, env, MAX_STEPS)
        
        pos_errors = (traj['follower_pos'] - traj['leader_pos'][:, np.newaxis])**2
        final_error = np.mean(pos_errors[-1])
        avg_error = np.mean(pos_errors)
        
        results.append({'final_error': final_error, 'avg_error': avg_error})
        
        ax1 = axes[test_idx, 0]
        ax1.plot(traj['times'], traj['leader_pos'], 'r-', linewidth=2.5, label='Leader')
        colors = plt.cm.Blues(np.linspace(0.3, 0.9, traj['follower_pos'].shape[1]))
        for i in range(min(5, traj['follower_pos'].shape[1])):
            ax1.plot(traj['times'], traj['follower_pos'][:, i], color=colors[i], 
                    alpha=0.8, linewidth=1.2, label=f'F{i+1}')
        ax1.set_xlabel('Time (s)')
        ax1.set_ylabel('Position')
        ax1.set_title(f'Test {test_idx+1}: Position (Final Err: {final_error:.4f})')
        ax1.legend(loc='upper right', fontsize=8)
        ax1.grid(True, alpha=0.3)
        
        ax2 = axes[test_idx, 1]
        ax2.plot(traj['times'], traj['leader_vel'], 'r-', linewidth=2.5, label='Leader')
        for i in range(min(5, traj['follower_vel'].shape[1])):
            ax2.plot(traj['times'], traj['follower_vel'][:, i], color=colors[i], 
                    alpha=0.8, linewidth=1.2)
        ax2.set_xlabel('Time (s)')
        ax2.set_ylabel('Velocity')
        ax2.set_title(f'Test {test_idx+1}: Velocity')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"üìÅ Figure saved to {save_path}")
    
    plt.show()
    
    print("\nüìä Evaluation Results:")
    print("-" * 40)
    for i, r in enumerate(results):
        print(f"Test {i+1}: Final Err = {r['final_error']:.4f}, Avg Err = {r['avg_error']:.4f}")
    
    return results"""
ËÆ≠ÁªÉÂèØËßÜÂåñ‰ª™Ë°®Áõò - ‰øÆÂ§çÁâà
"""
import time
import numpy as np

try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.rcParams['figure.max_open_warning'] = 50
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    HAS_WIDGETS = True
except ImportError:
    HAS_WIDGETS = False


class TrainingDashboard:
    """ËÆ≠ÁªÉ‰ª™Ë°®Áõò - ‰øÆÂ§çÁâà"""
    
    def __init__(self, total_episodes, vis_interval=10):
        self.total_episodes = total_episodes
        self.vis_interval = vis_interval
        self.start_time = None
        
        # ÂéÜÂè≤ËÆ∞ÂΩï
        self.reward_history = []
        self.tracking_error_history = []
        self.comm_history = []
        self.best_reward = -float('inf')
        self.best_trajectory = None
        
        self.use_widgets = HAS_WIDGETS and HAS_MATPLOTLIB
        
        if self.use_widgets:
            self._create_widgets()
    
    def _create_widgets(self):
        """ÂàõÂª∫ UI ÁªÑ‰ª∂"""
        self.title_html = widgets.HTML(value="""
            <div style="background: linear-gradient(90deg, #11998e 0%, #38ef7d 100%); 
                        padding: 15px; border-radius: 10px; margin-bottom: 10px;">
                <h2 style="color: white; margin: 0; text-align: center;">
                    üéØ Leader-Follower MAS Consensus Control
                </h2>
            </div>
        """)
        
        self.main_progress = widgets.FloatProgress(
            value=0, min=0, max=100, description='Total:',
            bar_style='info', style={'bar_color': '#11998e', 'description_width': '60px'},
            layout=widgets.Layout(width='100%', height='30px')
        )
        
        self.step_progress = widgets.FloatProgress(
            value=0, min=0, max=100, description='Episode:',
            bar_style='success', style={'bar_color': '#38ef7d', 'description_width': '60px'},
            layout=widgets.Layout(width='100%', height='20px')
        )
        
        self.progress_text = widgets.HTML(value="<p>Initializing...</p>")
        self.stats_html = widgets.HTML(value="")
        self.plot_output = widgets.Output()
        self.log_output = widgets.Output(layout=widgets.Layout(
            height='150px', overflow='auto', border='1px solid #ddd', padding='10px'
        ))
    
    def _format_time(self, seconds):
        """Ê†ºÂºèÂåñÊó∂Èó¥"""
        if seconds is None or seconds < 0:
            return "N/A"
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            return f"{seconds//60:.0f}m {seconds%60:.0f}s"
        return f"{seconds//3600:.0f}h {(seconds%3600)//60:.0f}m"
    
    def _get_elapsed(self):
        """Ëé∑ÂèñÂ∑≤Áî®Êó∂Èó¥"""
        if self.start_time is None:
            return 0
        return time.time() - self.start_time
    
    def _estimate_remaining(self, episode, elapsed):
        """‰º∞ËÆ°Ââ©‰ΩôÊó∂Èó¥"""
        if episode == 0 or elapsed is None or elapsed <= 0:
            return "..."
        return self._format_time((elapsed / episode) * (self.total_episodes - episode))
    
    def _generate_stats_html(self, episode, reward, tracking_err, comm, best, losses, elapsed):
        """ÁîüÊàêÁªüËÆ°‰ø°ÊÅØ HTML"""
        r_color = "#48bb78" if reward > -500 else "#f56565" if reward < -1500 else "#ed8936"
        e_color = "#48bb78" if tracking_err < 0.5 else "#f56565" if tracking_err > 2 else "#ed8936"
        c_color = "#48bb78" if comm < 0.3 else "#f56565" if comm > 0.6 else "#ed8936"
        
        return f"""
        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin: 10px 0;">
            <div style="flex:1;min-width:100px;background:linear-gradient(135deg,#11998e,#38ef7d);padding:10px;border-radius:8px;color:white;text-align:center;">
                <div style="font-size:11px;">üìç Episode</div>
                <div style="font-size:18px;font-weight:bold;">{episode}/{self.total_episodes}</div>
            </div>
            <div style="flex:1;min-width:100px;background:{r_color};padding:10px;border-radius:8px;color:white;text-align:center;">
                <div style="font-size:11px;">üèÜ Reward</div>
                <div style="font-size:18px;font-weight:bold;">{reward:.2f}</div>
                <div style="font-size:9px;">Best: {best:.2f}</div>
            </div>
            <div style="flex:1;min-width:100px;background:{e_color};padding:10px;border-radius:8px;color:white;text-align:center;">
                <div style="font-size:11px;">üéØ Error</div>
                <div style="font-size:18px;font-weight:bold;">{tracking_err:.4f}</div>
            </div>
            <div style="flex:1;min-width:100px;background:{c_color};padding:10px;border-radius:8px;color:white;text-align:center;">
                <div style="font-size:11px;">üì° Comm</div>
                <div style="font-size:18px;font-weight:bold;">{comm*100:.1f}%</div>
            </div>
            <div style="flex:1;min-width:100px;background:#4a5568;padding:10px;border-radius:8px;color:white;text-align:center;">
                <div style="font-size:11px;">‚è±Ô∏è Time</div>
                <div style="font-size:18px;font-weight:bold;">{self._format_time(elapsed)}</div>
                <div style="font-size:9px;">ETA: {self._estimate_remaining(episode, elapsed)}</div>
            </div>
        </div>
        <div style="background:#f7fafc;padding:6px;border-radius:6px;font-size:11px;">
            Q1: <b>{losses.get('q1',0):.4f}</b> | Q2: <b>{losses.get('q2',0):.4f}</b> | 
            Actor: <b>{losses.get('actor',0):.4f}</b> | Œ±: <b>{losses.get('alpha',0.2):.4f}</b>
        </div>
        """
    
    def display(self):
        """ÊòæÁ§∫‰ª™Ë°®Áõò"""
        self.start_time = time.time()
        if self.use_widgets:
            dashboard = widgets.VBox([
                self.title_html, self.main_progress, self.step_progress,
                self.progress_text, self.stats_html,
                widgets.HTML("<h4>üìà Training Progress</h4>"),
                self.plot_output,
                widgets.HTML("<h4>üìù Log</h4>"),
                self.log_output
            ])
            display(dashboard)
        else:
            print("Dashboard requires ipywidgets in Jupyter environment")
            print("Falling back to console output...")
    
    def update_step(self, step, max_steps):
        """Êõ¥Êñ∞Ê≠•Êï∞ËøõÂ∫¶"""
        if self.use_widgets:
            self.step_progress.value = (step / max_steps) * 100
    
    def update_episode(self, episode, reward, tracking_err, comm, losses, trajectory_data=None):
        """Êõ¥Êñ∞ÂõûÂêà‰ø°ÊÅØ"""
        elapsed = self._get_elapsed()
        
        # ËÆ∞ÂΩïÂéÜÂè≤
        self.reward_history.append(reward)
        self.tracking_error_history.append(tracking_err)
        self.comm_history.append(comm)
        
        # Êõ¥Êñ∞ÊúÄ‰Ω≥ËÆ∞ÂΩï
        if reward > self.best_reward:
            self.best_reward = reward
            if trajectory_data is not None:
                self.best_trajectory = trajectory_data
        
        if self.use_widgets:
            # Êõ¥Êñ∞ËøõÂ∫¶Êù°
            self.main_progress.value = (episode / self.total_episodes) * 100
            self.step_progress.value = 0
            
            # Êõ¥Êñ∞ÊñáÊú¨
            speed = episode / elapsed if elapsed > 0 else 0
            self.progress_text.value = f"<p>üöÄ <b>Ep {episode}</b> | {speed:.2f} ep/s</p>"
            self.stats_html.value = self._generate_stats_html(
                episode, reward, tracking_err, comm, self.best_reward, losses, elapsed
            )
            
            # Êõ¥Êñ∞Êó•Âøó
            with self.log_output:
                ts = time.strftime("%H:%M:%S")
                st = "üèÜ" if reward >= self.best_reward - 0.1 else "‚úÖ" if reward > -10 else "‚ö†Ô∏è"
                print(f"[{ts}] {st} Ep {episode:4d} | R:{reward:7.2f} | Err:{tracking_err:.4f} | Comm:{comm*100:.1f}%")
            
            # Êõ¥Êñ∞ÂõæË°®
            if episode % self.vis_interval == 0 or episode == 1:
                self._update_plots()
        else:
            if episode % 20 == 0:
                print(f"Ep {episode:4d} | R:{reward:7.2f} | Err:{tracking_err:.4f} | Comm:{comm*100:.1f}%")
    
    def _update_plots(self):
        """Êõ¥Êñ∞ËÆ≠ÁªÉÂõæË°® - ‰øÆÂ§çÁâà"""
        if not HAS_MATPLOTLIB:
            return
        
        with self.plot_output:
            clear_output(wait=True)
            
            # üîß ‰ΩøÁî® constrained_layout Êõø‰ª£ tight_layout
            fig, axes = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)
            
            # È¢úËâ≤ÂÆö‰πâ
            leader_color = '#e74c3c'
            raw_color = '#95a5a6'
            smooth_color = '#11998e'
            error_color = '#f39c12'
            comm_color = '#e74c3c'
            
            # ========== Â≠êÂõæ 1: ‰ΩçÁΩÆË∑üË∏™ ==========
            ax1 = axes[0, 0]
            if self.best_trajectory is not None:
                t = self.best_trajectory['times']
                fp = self.best_trajectory['follower_pos']
                lp = self.best_trajectory['leader_pos']
                num_followers = fp.shape[1]
                
                # ÁªòÂà∂ followers
                colors = plt.cm.Blues(np.linspace(0.4, 0.9, num_followers))
                for i in range(num_followers):
                    label = 'Followers' if i == 0 else None
                    ax1.plot(t, fp[:, i], color=colors[i], alpha=0.6, lw=1.0, label=label)
                
                # ÁªòÂà∂ leader
                ax1.plot(t, lp, color=leader_color, lw=2.5, label='Leader', zorder=10)
                
                # ÁªòÂà∂Âπ≥Âùá follower
                avg_fp = fp.mean(axis=1)
                ax1.plot(t, avg_fp, color='#2ecc71', lw=2, linestyle='--', 
                        label='Avg Follower', alpha=0.8, zorder=9)
            
            ax1.set_title(f'Position Tracking (Best R={self.best_reward:.2f})', fontsize=12, fontweight='bold')
            ax1.set_xlabel('Time (s)', fontsize=10)
            ax1.set_ylabel('Position', fontsize=10)
            ax1.legend(loc='upper right', fontsize=9)
            ax1.grid(True, alpha=0.3)
            
            # ========== Â≠êÂõæ 2: ÈÄüÂ∫¶Ë∑üË∏™ ==========
            ax2 = axes[0, 1]
            if self.best_trajectory is not None:
                t = self.best_trajectory['times']
                fv = self.best_trajectory['follower_vel']
                lv = self.best_trajectory['leader_vel']
                num_followers = fv.shape[1]
                
                colors = plt.cm.Blues(np.linspace(0.4, 0.9, num_followers))
                for i in range(num_followers):
                    label = 'Followers' if i == 0 else None
                    ax2.plot(t, fv[:, i], color=colors[i], alpha=0.6, lw=1.0, label=label)
                
                ax2.plot(t, lv, color=leader_color, lw=2.5, label='Leader', zorder=10)
                
                avg_fv = fv.mean(axis=1)
                ax2.plot(t, avg_fv, color='#2ecc71', lw=2, linestyle='--', 
                        label='Avg Follower', alpha=0.8, zorder=9)
            
            ax2.set_title('Velocity Tracking', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Time (s)', fontsize=10)
            ax2.set_ylabel('Velocity', fontsize=10)
            ax2.legend(loc='upper right', fontsize=9)
            ax2.grid(True, alpha=0.3)
            
            # ========== Â≠êÂõæ 3: Â•ñÂä±Êõ≤Á∫ø ==========
            ax3 = axes[1, 0]
            num_eps = len(self.reward_history)
            
            if num_eps > 0:
                eps = np.arange(1, num_eps + 1)
                
                # ÁªòÂà∂ÂéüÂßãÂ•ñÂä±
                ax3.plot(eps, self.reward_history, color=raw_color, alpha=0.5, lw=1, 
                        label='Raw Reward')
                
                # ÁªòÂà∂Âπ≥ÊªëÂ•ñÂä±
                if num_eps >= 10:
                    w = min(20, num_eps // 2)
                    if w >= 2:
                        sm = np.convolve(self.reward_history, np.ones(w)/w, mode='valid')
                        sm_eps = np.arange(w, num_eps + 1)
                        ax3.plot(sm_eps, sm, color=smooth_color, lw=2.5, label=f'Smoothed (w={w})')
                
                # Ê†áËÆ∞ÊúÄ‰Ω≥
                best_idx = np.argmax(self.reward_history)
                ax3.scatter([best_idx + 1], [self.reward_history[best_idx]], 
                           color='gold', s=150, marker='*', zorder=15,
                           edgecolors='black', linewidths=0.5, label=f'Best: {self.best_reward:.2f}')
                
                # üîß ‰øÆÂ§ç x ËΩ¥ÂàªÂ∫¶
                ax3.set_xlim(0, max(num_eps + 1, 10))
                ax3.xaxis.set_major_locator(plt.MaxNLocator(integer=True))
            
            ax3.set_title('Episode Reward', fontsize=12, fontweight='bold')
            ax3.set_xlabel('Episode', fontsize=10)
            ax3.set_ylabel('Reward', fontsize=10)
            ax3.legend(loc='best', fontsize=9)
            ax3.grid(True, alpha=0.3)
            
            # Ê∑ªÂä†ÈÄö‰ø°ÁéáÂâØËΩ¥
            if num_eps > 0:
                ax3t = ax3.twinx()
                ax3t.plot(eps, [c*100 for c in self.comm_history], color=comm_color, 
                         linestyle=':', lw=1.5, alpha=0.7)
                ax3t.set_ylabel('Comm Rate (%)', color=comm_color, fontsize=10)
                ax3t.set_ylim(0, 100)
                ax3t.tick_params(axis='y', labelcolor=comm_color)
            
            # ========== Â≠êÂõæ 4: Ë∑üË∏™ËØØÂ∑Æ ==========
            ax4 = axes[1, 1]
            
            if num_eps > 0:
                eps = np.arange(1, num_eps + 1)
                
                ax4.plot(eps, self.tracking_error_history, color=error_color, alpha=0.5, lw=1, 
                        label='Raw Error')
                
                if num_eps >= 10:
                    w = min(20, num_eps // 2)
                    if w >= 2:
                        sme = np.convolve(self.tracking_error_history, np.ones(w)/w, mode='valid')
                        sme_eps = np.arange(w, num_eps + 1)
                        ax4.plot(sme_eps, sme, color='#38ef7d', lw=2.5, label=f'Smoothed (w={w})')
                
                # Ê†áËÆ∞ÊúÄÂ∞èËØØÂ∑Æ
                min_idx = np.argmin(self.tracking_error_history)
                min_err = self.tracking_error_history[min_idx]
                ax4.scatter([min_idx + 1], [min_err], 
                           color='lime', s=150, marker='*', zorder=15,
                           edgecolors='black', linewidths=0.5, label=f'Min: {min_err:.4f}')
                
                # üîß ‰øÆÂ§ç x ËΩ¥ÂàªÂ∫¶
                ax4.set_xlim(0, max(num_eps + 1, 10))
                ax4.xaxis.set_major_locator(plt.MaxNLocator(integer=True))
            
            ax4.set_title('Tracking Error', fontsize=12, fontweight='bold')
            ax4.set_xlabel('Episode', fontsize=10)
            ax4.set_ylabel('Error', fontsize=10)
            ax4.legend(loc='best', fontsize=9)
            ax4.grid(True, alpha=0.3)
            
            plt.show()
    
    def finish(self):
        """ËÆ≠ÁªÉÂÆåÊàê"""
        elapsed = self._get_elapsed()
        if self.use_widgets:
            self.main_progress.value = 100
            self.main_progress.bar_style = 'success'
            with self.log_output:
                print("=" * 50)
                print(f"‚úÖ Training Complete!")
                print(f"   Total Time: {self._format_time(elapsed)}")
                print(f"   Best Reward: {self.best_reward:.2f}")
                if self.tracking_error_history:
                    print(f"   Final Tracking Error: {self.tracking_error_history[-1]:.4f}")
                if self.comm_history:
                    print(f"   Final Comm Rate: {self.comm_history[-1]*100:.1f}%")
                print("=" * 50)
        else:
            print(f"\n‚úÖ Training complete!")
            print(f"   Best reward: {self.best_reward:.2f}")
            print(f"   Time: {self._format_time(elapsed)}")
    
    def get_summary(self):
        """Ëé∑ÂèñËÆ≠ÁªÉÊëòË¶Å"""
        return {
            'best_reward': self.best_reward,
            'final_reward': self.reward_history[-1] if self.reward_history else None,
            'final_tracking_error': self.tracking_error_history[-1] if self.tracking_error_history else None,
            'final_comm_rate': self.comm_history[-1] if self.comm_history else None,
            'total_episodes': len(self.reward_history),
            'elapsed_time': self._get_elapsed()
        }"""
ËÆ≠ÁªÉËÑöÊú¨ - ‰øÆÂ§çÁâà
"""
import torch
import time

from config import (
    NUM_FOLLOWERS, NUM_PINNED, MAX_STEPS, BATCH_SIZE,
    NUM_EPISODES, VIS_INTERVAL, SAVE_MODEL_PATH, 
    print_config, set_seed, SEED,
    NUM_PARALLEL_ENVS, UPDATE_FREQUENCY, GRADIENT_STEPS,
    USE_AMP, DEVICE
)
from topology import DirectedSpanningTreeTopology
from environment import BatchedLeaderFollowerEnv, LeaderFollowerMASEnv
from agent import SACAgent
from dashboard import TrainingDashboard
from utils import collect_trajectory, plot_evaluation


# CUDA ‰ºòÂåñ
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


def train(num_episodes=NUM_EPISODES, vis_interval=VIS_INTERVAL, 
          show_dashboard=True, seed=SEED):
    """ÈÄüÂ∫¶‰ºòÂåñËÆ≠ÁªÉ - ‰øÆÂ§çÁâà"""
    set_seed(seed)
    print_config()
    
    # ÂàùÂßãÂåñ
    topology = DirectedSpanningTreeTopology(NUM_FOLLOWERS, num_pinned=NUM_PINNED)
    batched_env = BatchedLeaderFollowerEnv(topology, num_envs=NUM_PARALLEL_ENVS)
    eval_env = LeaderFollowerMASEnv(topology)
    
    agent = SACAgent(topology, use_amp=USE_AMP)
    
    dashboard = None
    if show_dashboard:
        dashboard = TrainingDashboard(num_episodes, vis_interval)
        dashboard.display()
    
    best_reward = -float('inf')
    global_step = 0
    
    start_time = time.time()
    log_interval = 10
    
    # ËÆ≠ÁªÉÂæ™ÁéØ
    for episode in range(1, num_episodes + 1):
        states = batched_env.reset()
        
        episode_rewards = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)
        episode_tracking_err = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)
        episode_comm = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)
        
        for step in range(MAX_STEPS):
            global_step += NUM_PARALLEL_ENVS
            
            # üîß Êõ¥Êñ∞ Episode ËøõÂ∫¶Êù°
            if dashboard and step % 10 == 0:  # ÊØè10Ê≠•Êõ¥Êñ∞‰∏ÄÊ¨°ÔºåÈÅøÂÖçÂ§™È¢ëÁπÅ
                dashboard.update_step(step, MAX_STEPS)
            
            actions = agent.select_action(states, deterministic=False)
            next_states, rewards, dones, infos = batched_env.step(actions)
            
            agent.store_transitions_batch(states, actions, rewards, next_states, dones)
            
            # ÂáèÂ∞ëÊõ¥Êñ∞È¢ëÁéá
            if step % UPDATE_FREQUENCY == 0 and step > 0:
                agent.update(BATCH_SIZE, GRADIENT_STEPS)
            
            episode_rewards += rewards
            episode_tracking_err += infos['tracking_error']
            episode_comm += infos['comm_rate']
            states = next_states
        
        avg_reward = episode_rewards.mean().item()
        avg_tracking_err = (episode_tracking_err / MAX_STEPS).mean().item()
        avg_comm = (episode_comm / MAX_STEPS).mean().item()
        
        # ÂáèÂ∞ëÂèØËßÜÂåñÈ¢ëÁéá
        trajectory_data = None
        if episode % vis_interval == 0 or episode == 1:
            trajectory_data = collect_trajectory(agent, eval_env, MAX_STEPS)
        
        if avg_reward > best_reward:
            best_reward = avg_reward
            agent.save(SAVE_MODEL_PATH)
            trajectory_data = collect_trajectory(agent, eval_env, MAX_STEPS)
        
        if dashboard:
            dashboard.update_episode(
                episode, avg_reward, avg_tracking_err, avg_comm,
                agent.last_losses, trajectory_data
            )
        elif episode % log_interval == 0:
            elapsed = time.time() - start_time
            speed = episode / elapsed
            print(f"Ep {episode:4d} | R:{avg_reward:7.2f} | Err:{avg_tracking_err:.4f} | "
                  f"Comm:{avg_comm*100:.1f}% | Speed:{speed:.2f} ep/s | "
                  f"Steps:{global_step/1e6:.2f}M")
    
    if dashboard:
        dashboard.finish()
    
    elapsed = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"‚úÖ Training Complete!")
    print(f"   Total time: {elapsed:.1f}s ({elapsed/60:.1f} min)")
    print(f"   Speed: {num_episodes/elapsed:.2f} ep/s")
    print(f"   Total steps: {global_step:,}")
    print(f"   Best reward: {best_reward:.2f}")
    print(f"{'='*60}")
    
    return agent, topology, dashboard


if __name__ == '__main__':
    agent, topology, _ = train(show_dashboard=False)
    plot_evaluation(agent, topology, num_tests=3, save_path='evaluation.png')