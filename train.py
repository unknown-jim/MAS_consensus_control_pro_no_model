"""CTDE è®­ç»ƒå…¥å£ã€‚

æœ¬è„šæœ¬æä¾›ä¸€ä¸ªå¯ç›´æ¥è¿è¡Œçš„è®­ç»ƒå…¥å£ï¼šé›†ä¸­è®­ç»ƒã€åˆ†æ•£æ‰§è¡Œï¼ˆCTDEï¼‰ã€‚
- Actorï¼šä»…ä½¿ç”¨æœ¬åœ°è§‚æµ‹ï¼ˆæ¯ä¸ª follower ç‹¬ç«‹å†³ç­–ï¼‰
- Critic/Valueï¼šä½¿ç”¨å…¨å±€çŠ¶æ€ï¼ˆä»¥åŠ SAC çš„è”åˆåŠ¨ä½œï¼‰

æ³¨æ„ï¼š`show_dashboard=True` æ—¶ä¼šåœ¨è¿è¡ŒæœŸå¯¼å…¥ `mas_cc.dashboard`ï¼Œéœ€è¦åœ¨ Jupyter ç¯å¢ƒä¸‹ä½¿ç”¨ã€‚
"""
import torch
import time

from mas_cc.config import (
    NUM_FOLLOWERS, NUM_PINNED, MAX_STEPS, BATCH_SIZE,
    NUM_EPISODES, VIS_INTERVAL, SAVE_MODEL_PATH,
    print_config, set_seed, SEED,
    NUM_PARALLEL_ENVS, UPDATE_FREQUENCY, GRADIENT_STEPS,
    USE_AMP, DEVICE, COMM_PENALTY, THRESHOLD_MIN, THRESHOLD_MAX,
    WARMUP_STEPS,

    ALGO, PPO_ROLLOUT_STEPS,
)
from mas_cc.topology import CommunicationTopology
from mas_cc.environment import BatchedModelFreeEnv, ModelFreeEnv
from mas_cc.agent import CTDESACAgent, CTDEMAPPOAgent
from mas_cc.utils import collect_trajectory, plot_evaluation


torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# PyTorch 2.xï¼šæå‡ matmul ç²¾åº¦/æ€§èƒ½ï¼ˆå¯¹ Transformer/MLP å¸¸æœ‰æ”¶ç›Šï¼‰
torch.set_float32_matmul_precision("high")


def train(
    num_episodes=NUM_EPISODES,
    vis_interval=VIS_INTERVAL,
    show_dashboard: bool = False,
    seed=SEED,
    profile_timing: bool = False,
):
    """è¿è¡Œ CTDE è®­ç»ƒã€‚

    Args:
        num_episodes: è®­ç»ƒ episode æ•°ã€‚
        vis_interval: å¯è§†åŒ–/è½¨è¿¹é‡‡æ ·çš„é—´éš”ï¼ˆæ¯éš”å¤šå°‘ä¸ª episode é‡‡æ ·ä¸€æ¬¡ï¼‰ã€‚
        show_dashboard: æ˜¯å¦å¯ç”¨ Jupyter ä»ªè¡¨ç›˜ï¼›å¯ç”¨ä¼šé¢å¤–ä¾èµ– `ipywidgets` ç­‰ã€‚
        seed: éšæœºç§å­ã€‚
        profile_timing: æ˜¯å¦ç»Ÿè®¡ç²—ç²’åº¦è€—æ—¶ï¼ˆstep/update çš„ç´¯è®¡å‡å€¼ï¼‰ã€‚

    Returns:
        (agent, topology, dashboard)ï¼š
        - agent: è®­ç»ƒå¾—åˆ°çš„æ™ºèƒ½ä½“å®ä¾‹ï¼ˆ`CTDESACAgent` æˆ– `CTDEMAPPOAgent`ï¼‰ã€‚
        - topology: æœ¬æ¬¡è®­ç»ƒä½¿ç”¨çš„ `CommunicationTopology` å®ä¾‹ã€‚
        - dashboard: è‹¥å¯ç”¨ä»ªè¡¨ç›˜è¿”å› `TrainingDashboard`ï¼Œå¦åˆ™ä¸º `None`ã€‚
    """
    set_seed(seed)
    print_config()
    
    print("\n" + "="*60)
    print("ğŸš€ CTDE Training (Centralized Training Decentralized Execution)")
    print(f"   â€¢ Algorithm: {ALGO}")
    print("   â€¢ Actor: Decentralized (local observation only)")
    print("   â€¢ Critic: Centralized (global state + joint action)")
    print("   â€¢ Execution: Each agent uses only local information")
    print(f"   â€¢ Warmup Steps: {WARMUP_STEPS}")
    print("="*60)
    print(f"\nğŸ“¡ Communication Settings:")
    print(f"   Comm Penalty: {COMM_PENALTY}")
    print(f"   Threshold Range: [{THRESHOLD_MIN}, {THRESHOLD_MAX}]")
    print()
    
    # åˆå§‹åŒ–
    topology = CommunicationTopology(NUM_FOLLOWERS, num_pinned=NUM_PINNED)
    batched_env = BatchedModelFreeEnv(topology, num_envs=NUM_PARALLEL_ENVS)
    eval_env = ModelFreeEnv(topology)
    
    algo = str(ALGO).upper().strip()
    is_mappo = (algo == 'MAPPO')

    if is_mappo:
        agent = CTDEMAPPOAgent(topology, use_amp=False)
        print(f"   â€¢ MAPPO Rollout Steps: {PPO_ROLLOUT_STEPS}")
    else:
        # é»˜è®¤ MASACï¼ˆCTDE-SACï¼‰
        agent = CTDESACAgent(topology, use_amp=USE_AMP)
    
    dashboard = None
    if show_dashboard:
        # ä»…åœ¨éœ€è¦å¯è§†åŒ–æ—¶å¯¼å…¥ï¼ˆä¾èµ– Jupyter + ipywidgetsï¼‰
        from mas_cc.dashboard import TrainingDashboard

        dashboard = TrainingDashboard(num_episodes, vis_interval, topology=topology)
        dashboard.display()
    
    best_reward = -float('inf')
    best_model_state = None  # è®°å½•æœ€ä¼˜æ¨¡å‹çŠ¶æ€ï¼ˆå†…å­˜ä¸­ï¼‰
    global_step = 0
    
    start_time = time.time()
    log_interval = 10

    # ä½æˆæœ¬ profilingï¼ˆé»˜è®¤å…³é—­ï¼‰
    step_time_s = 0.0
    update_time_s = 0.0
    update_calls = 0

    for episode in range(1, num_episodes + 1):
        
        local_states = batched_env.reset()
        # CTDEï¼šcritic/value çš„è¾“å…¥ï¼ˆä¸ actor çš„æœ¬åœ°è¾“å…¥åŒºåˆ†å¼€ï¼‰
        global_states = batched_env.get_global_state()

        episode_rewards = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)
        episode_tracking_err = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)
        episode_comm = torch.zeros(NUM_PARALLEL_ENVS, device=DEVICE)

        for step in range(MAX_STEPS):
            global_step += NUM_PARALLEL_ENVS

            if dashboard and step % 10 == 0:
                dashboard.update_step(step, MAX_STEPS)

            # æ‰§è¡Œé˜¶æ®µï¼šactor åªä½¿ç”¨æœ¬åœ°çŠ¶æ€ï¼›è®­ç»ƒé˜¶æ®µï¼šMAPPO è¿˜ä¼šç”¨ global_state ä¼°è®¡ value
            if is_mappo:
                actions, logp_joint, values = agent.act(local_states, global_states, deterministic=False)
            else:
                actions = agent.select_action(local_states, deterministic=False)

            if profile_timing:
                t0 = time.perf_counter()
            next_local_states, rewards, dones, infos = batched_env.step(actions)
            next_global_states = batched_env.get_global_state()
            if profile_timing:
                step_time_s += (time.perf_counter() - t0)

            # å¤„ç†æ—¶é—´æˆªæ–­ï¼šæœ€åä¸€æ­¥è§†ä¸ºç»ˆæ­¢ï¼Œé¿å…è·¨ episode çš„ bootstrapping åå·®
            time_limit_done = torch.zeros_like(dones)
            if step == MAX_STEPS - 1:
                time_limit_done[:] = True
            store_dones = dones | time_limit_done

            if is_mappo:
                # MAPPOï¼šon-policy rollout
                agent.store_rollout_step(
                    local_states, global_states, actions, logp_joint, values,
                    rewards, store_dones
                )

                do_update = agent.buffer.is_full() or (step == MAX_STEPS - 1)
                if do_update:
                    if profile_timing:
                        t1 = time.perf_counter()
                    agent.update(next_global_states=next_global_states, next_dones=store_dones)
                    if profile_timing:
                        update_time_s += (time.perf_counter() - t1)
                        update_calls += 1
            else:
                # MASACï¼ˆCTDE-SACï¼‰ï¼šoff-policy replay
                agent.store_transitions_batch(
                    local_states, global_states, actions, rewards,
                    next_local_states, next_global_states, store_dones
                )

                if step % UPDATE_FREQUENCY == 0 and step > 0 and global_step > WARMUP_STEPS:
                    if profile_timing:
                        t1 = time.perf_counter()
                    agent.update(BATCH_SIZE, GRADIENT_STEPS)
                    if profile_timing:
                        update_time_s += (time.perf_counter() - t1)
                        update_calls += 1
            
            episode_rewards += rewards
            episode_tracking_err += infos['tracking_error']
            episode_comm += infos['comm_rate']
            
            local_states = next_local_states
            global_states = next_global_states
        
        avg_reward = episode_rewards.mean().item()
        avg_tracking_err = (episode_tracking_err / MAX_STEPS).mean().item()
        avg_comm = (episode_comm / MAX_STEPS).mean().item()
        
        trajectory_data = None
        if episode % vis_interval == 0 or episode == 1:
            trajectory_data = collect_trajectory(agent, eval_env, MAX_STEPS)
        
        if avg_reward > best_reward:
            best_reward = avg_reward
            # è®°å½•æœ€ä¼˜æ¨¡å‹çŠ¶æ€ï¼ˆå†…å­˜ä¸­ï¼‰ï¼Œè®­ç»ƒç»“æŸåå†è½ç›˜
            best_model_state = {
                'actor': {k: v.clone() for k, v in agent.actor.state_dict().items()},
            }
            if is_mappo:
                best_model_state['value'] = {k: v.clone() for k, v in agent.value_net.state_dict().items()}
            else:
                best_model_state['q1'] = {k: v.clone() for k, v in agent.q1.state_dict().items()}
                best_model_state['q2'] = {k: v.clone() for k, v in agent.q2.state_dict().items()}
                best_model_state['q1_target'] = {k: v.clone() for k, v in agent.q1_target.state_dict().items()}
                best_model_state['q2_target'] = {k: v.clone() for k, v in agent.q2_target.state_dict().items()}
            trajectory_data = collect_trajectory(agent, eval_env, MAX_STEPS)
        
        if dashboard:
            dashboard.update_episode(
                episode, avg_reward, avg_tracking_err, avg_comm,
                agent.last_losses, trajectory_data
            )
        elif episode % log_interval == 0:
            elapsed = time.time() - start_time
            speed = episode / elapsed

            if profile_timing:
                # ä»¥â€œæ¯ episodeâ€å±•ç¤ºä¸€ä¸ªå¤§è‡´å æ¯”ï¼ˆè·¨ episode ç´¯è®¡çš„å‡å€¼ï¼‰
                avg_step_ms = (step_time_s / max(1, episode)) * 1000
                avg_update_ms = (update_time_s / max(1, episode)) * 1000
                upd_per_ep = update_calls / max(1, episode)
                timing_str = f" | step:{avg_step_ms:.0f}ms/ep | upd:{avg_update_ms:.0f}ms/ep ({upd_per_ep:.2f}/ep)"
            else:
                timing_str = ""

            print(f"Ep {episode:4d} | R:{avg_reward:7.2f} | Err:{avg_tracking_err:.4f} | "
                  f"Comm:{avg_comm*100:.1f}% | {speed:.2f} ep/s{timing_str}")
    
    if dashboard:
        dashboard.finish()
    
    # è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°ç£ç›˜
    if best_model_state is not None:
        import os
        parent = os.path.dirname(SAVE_MODEL_PATH)
        if parent:
            os.makedirs(parent, exist_ok=True)
        
        if is_mappo:
            torch.save({
                'actor': best_model_state['actor'],
                'value': best_model_state['value'],
            }, SAVE_MODEL_PATH)
        else:
            torch.save({
                'actor': best_model_state['actor'],
                'q1': best_model_state['q1'],
                'q2': best_model_state['q2'],
                'q1_target': best_model_state['q1_target'],
                'q2_target': best_model_state['q2_target'],
            }, SAVE_MODEL_PATH)
        print(f"âœ… Best model saved to {SAVE_MODEL_PATH}")
    
    elapsed = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"âœ… CTDE Training Complete!")
    print(f"   Total time: {elapsed:.1f}s ({elapsed/60:.1f} min)")
    print(f"   Speed: {num_episodes/elapsed:.2f} ep/s")
    print(f"   Total steps: {global_step:,}")
    print(f"   Best reward: {best_reward:.2f}")
    print(f"{'='*60}")
    
    return agent, topology, dashboard


if __name__ == '__main__':
    agent, topology, _ = train(show_dashboard=False)
    # ç»Ÿä¸€ä» config è¯»å–è¯„ä¼°ä¿å­˜è·¯å¾„ï¼ˆè‡ªåŠ¨è½åˆ° results/.../figs/ï¼‰
    from mas_cc.config import EVAL_NUM_TESTS, EVAL_SAVE_PATH
    plot_evaluation(agent, topology, num_tests=EVAL_NUM_TESTS, save_path=EVAL_SAVE_PATH)
