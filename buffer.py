"""
ç»éªŒå›æ”¾ç¼“å†²åŒº - CTDE ç‰ˆæœ¬

å­˜å‚¨ï¼š
- æœ¬åœ°çŠ¶æ€ï¼ˆç”¨äº Actorï¼‰
- å…¨å±€çŠ¶æ€ï¼ˆç”¨äº Criticï¼‰
- è”åˆåŠ¨ä½œ
- å¥–åŠ±
"""
import torch
from config import DEVICE, BUFFER_SIZE, STATE_DIM, ACTION_DIM, NUM_AGENTS, GLOBAL_STATE_DIM, NUM_FOLLOWERS


class CTDEReplayBuffer:
    """CTDE æ¶æ„çš„ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity=BUFFER_SIZE, num_agents=NUM_AGENTS, 
                 state_dim=STATE_DIM, action_dim=ACTION_DIM,
                 global_state_dim=GLOBAL_STATE_DIM):
        self.capacity = capacity
        self.num_agents = num_agents
        self.num_followers = num_agents - 1
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.global_state_dim = global_state_dim
        
        self.ptr = 0
        self.size = 0
        
        # é¢„åˆ†é… GPU å†…å­˜
        # æœ¬åœ°çŠ¶æ€ï¼ˆç”¨äº Actorï¼‰
        self.local_states = torch.zeros(capacity, num_agents, state_dim, device=DEVICE)
        self.next_local_states = torch.zeros(capacity, num_agents, state_dim, device=DEVICE)
        
        # ğŸ”§ å…¨å±€çŠ¶æ€ï¼ˆç”¨äº Criticï¼‰
        self.global_states = torch.zeros(capacity, global_state_dim, device=DEVICE)
        self.next_global_states = torch.zeros(capacity, global_state_dim, device=DEVICE)
        
        # è”åˆåŠ¨ä½œ
        self.actions = torch.zeros(capacity, self.num_followers, action_dim, device=DEVICE)
        
        # å¥–åŠ±å’Œç»ˆæ­¢æ ‡å¿—
        self.rewards = torch.zeros(capacity, device=DEVICE)
        self.dones = torch.zeros(capacity, device=DEVICE)
    
    def push_batch(self, local_states, global_states, actions, rewards, 
                   next_local_states, next_global_states, dones):
        """æ‰¹é‡å­˜å‚¨ç»éªŒ"""
        batch_size = local_states.shape[0]
        
        if self.ptr + batch_size <= self.capacity:
            idx = slice(self.ptr, self.ptr + batch_size)
            self.local_states[idx] = local_states
            self.global_states[idx] = global_states
            self.actions[idx] = actions
            self.rewards[idx] = rewards
            self.next_local_states[idx] = next_local_states
            self.next_global_states[idx] = next_global_states
            self.dones[idx] = dones.float()
        else:
            first_part = self.capacity - self.ptr
            second_part = batch_size - first_part
            
            # æœ¬åœ°çŠ¶æ€
            self.local_states[self.ptr:] = local_states[:first_part]
            self.local_states[:second_part] = local_states[first_part:]
            
            self.next_local_states[self.ptr:] = next_local_states[:first_part]
            self.next_local_states[:second_part] = next_local_states[first_part:]
            
            # å…¨å±€çŠ¶æ€
            self.global_states[self.ptr:] = global_states[:first_part]
            self.global_states[:second_part] = global_states[first_part:]
            
            self.next_global_states[self.ptr:] = next_global_states[:first_part]
            self.next_global_states[:second_part] = next_global_states[first_part:]
            
            # åŠ¨ä½œ
            self.actions[self.ptr:] = actions[:first_part]
            self.actions[:second_part] = actions[first_part:]
            
            # å¥–åŠ±
            self.rewards[self.ptr:] = rewards[:first_part]
            self.rewards[:second_part] = rewards[first_part:]
            
            # ç»ˆæ­¢
            self.dones[self.ptr:] = dones[:first_part].float()
            self.dones[:second_part] = dones[first_part:].float()
        
        self.ptr = (self.ptr + batch_size) % self.capacity
        self.size = min(self.size + batch_size, self.capacity)
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·"""
        indices = torch.randint(0, self.size, (batch_size,), device=DEVICE)
        
        return (
            self.local_states[indices],      # (batch, num_agents, state_dim)
            self.global_states[indices],      # (batch, global_state_dim)
            self.actions[indices],            # (batch, num_followers, action_dim)
            self.rewards[indices],            # (batch,)
            self.next_local_states[indices],  # (batch, num_agents, state_dim)
            self.next_global_states[indices], # (batch, global_state_dim)
            self.dones[indices]               # (batch,)
        )
    
    def __len__(self):
        return self.size
    
    def is_ready(self, batch_size):
        return self.size >= batch_size


# ä¿ç•™æ—§åç§°ä»¥å…¼å®¹
OptimizedReplayBuffer = CTDEReplayBuffer