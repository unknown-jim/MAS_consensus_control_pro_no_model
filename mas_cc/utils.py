"""è¯„ä¼°ä¸å¯è§†åŒ–å·¥å…·ï¼ˆCTDE ç‰ˆæœ¬ï¼‰ã€‚

è¯¥æ¨¡å—ä¸»è¦æä¾›ï¼š
- è½¨è¿¹é‡‡æ ·ï¼ˆç”¨äºç»˜å›¾/è¯Šæ–­ï¼‰
- å¤šæ¬¡è¯„ä¼°æ±‡æ€»
- è¯„ä¼°ç»˜å›¾ï¼ˆposition/velocity æ›²çº¿ï¼‰

çº¦å®šï¼šè¯„ä¼°é˜¶æ®µé»˜è®¤ä½¿ç”¨ deterministic åŠ¨ä½œã€‚
"""

from __future__ import annotations

import os

import matplotlib.pyplot as plt
import numpy as np
import torch

from .config import DEVICE, MAX_STEPS, THRESHOLD_MAX, THRESHOLD_MIN, TH_SCALE
from .environment import ModelFreeEnv


def _set_eval_for_inference(agent):
    """åœ¨è¯„ä¼°/å¯è§†åŒ–é˜¶æ®µä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼ã€‚

    Args:
        agent: æ™ºèƒ½ä½“å®ä¾‹ï¼Œéœ€è‡³å°‘åŒ…å« `actor`ï¼›è‹¥åŒ…å« `value_net` ä¹Ÿä¼šä¸€å¹¶å¤„ç†ã€‚

    Returns:
        ä¸€ä¸ªæ— å‚å‡½æ•°ï¼Œè°ƒç”¨åä¼šæŠŠæ¨¡å—çš„ train/eval çŠ¶æ€æ¢å¤åˆ°è¿›å…¥æœ¬å‡½æ•°ä¹‹å‰çš„çŠ¶æ€ã€‚
    """

    modules = [(agent.actor, bool(agent.actor.training))]
    if agent.value_net is not None:
        modules.append((agent.value_net, bool(agent.value_net.training)))

    for m, _ in modules:
        m.eval()

    def _restore():
        for m, was_training in modules:
            m.train(was_training)

    return _restore


@torch.no_grad()
def collect_trajectory(agent, env, max_steps: int = MAX_STEPS):
    """é‡‡æ ·ä¸€æ¡è½¨è¿¹ç”¨äºå¯è§†åŒ–ï¼ˆåŒ…å«é€šä¿¡ç‡ä¸é˜ˆå€¼ï¼‰ã€‚

    Args:
        agent: æ™ºèƒ½ä½“å®ä¾‹ï¼ˆéœ€å®ç° `select_action(state, deterministic=True)`ï¼‰ã€‚
        env: ç¯å¢ƒå®ä¾‹ï¼ˆé€šå¸¸ä¸º `ModelFreeEnv`ï¼‰ã€‚
        max_steps: é‡‡æ ·æ­¥æ•°ã€‚

    Returns:
        è½¨è¿¹å­—å…¸ï¼ŒåŒ…å«ï¼š
        - `times`: shape=(T+1,)
        - `leader_pos/leader_vel`: shape=(T+1,)
        - `follower_pos/follower_vel`: shape=(T+1, num_followers)
        - `comm_rates`: shape=(T,)
        - `thresholds`: shape=(T, num_followers)ï¼ˆç¯å¢ƒå®é™…é˜ˆå€¼ï¼‰
    """

    restore = _set_eval_for_inference(agent)
    state = env.reset()

    times = [0]
    leader_pos = [env.positions[0].item()]
    leader_vel = [env.velocities[0].item()]
    follower_pos = [env.positions[1:].cpu().numpy()]
    follower_vel = [env.velocities[1:].cpu().numpy()]

    comm_rates = []
    thresholds = []

    for _ in range(int(max_steps)):
        if state.dim() == 1:
            state = state.unsqueeze(0)

        action = agent.select_action(state, deterministic=True)
        state, _, _, info = env.step(action)

        times.append(env.t)
        leader_pos.append(env.positions[0].item())
        leader_vel.append(env.velocities[0].item())
        follower_pos.append(env.positions[1:].cpu().numpy())
        follower_vel.append(env.velocities[1:].cpu().numpy())

        comm_rates.append(info["comm_rate"])

        th_raw = action[:, 1]
        th_norm = (th_raw / TH_SCALE).clamp(0.0, 1.0)
        th_env = THRESHOLD_MIN + (THRESHOLD_MAX - THRESHOLD_MIN) * th_norm
        thresholds.append(th_env.cpu().numpy())

    out = {
        "times": np.array(times),
        "leader_pos": np.array(leader_pos),
        "leader_vel": np.array(leader_vel),
        "follower_pos": np.array(follower_pos),
        "follower_vel": np.array(follower_vel),
        "comm_rates": np.array(comm_rates),
        "thresholds": np.array(thresholds),
    }

    restore()
    return out


@torch.no_grad()
def evaluate_agent(agent, env, num_episodes: int = 5):
    """å¤šæ¬¡è¿è¡Œç¯å¢ƒå¹¶æ±‡æ€»è¯„ä¼°æŒ‡æ ‡ã€‚

    Args:
        agent: æ™ºèƒ½ä½“å®ä¾‹ã€‚
        env: ç¯å¢ƒå®ä¾‹ã€‚
        num_episodes: è¯„ä¼° episode æ•°ã€‚

    Returns:
        æŒ‡æ ‡å­—å…¸ï¼š`mean_reward/std_reward/mean_tracking_error/mean_comm_rate`ã€‚
    """

    restore = _set_eval_for_inference(agent)
    results = {"rewards": [], "tracking_errors": [], "comm_rates": []}

    for _ in range(int(num_episodes)):
        state = env.reset()
        episode_reward = 0
        episode_tracking_err = 0
        episode_comm = 0

        for _ in range(int(MAX_STEPS)):
            action = agent.select_action(state, deterministic=True)
            state, reward, _, info = env.step(action)

            episode_reward += reward
            episode_tracking_err += info["tracking_error"]
            episode_comm += info["comm_rate"]

        results["rewards"].append(episode_reward)
        results["tracking_errors"].append(episode_tracking_err / MAX_STEPS)
        results["comm_rates"].append(episode_comm / MAX_STEPS)

    out = {
        "mean_reward": float(np.mean(results["rewards"])) ,
        "std_reward": float(np.std(results["rewards"])) ,
        "mean_tracking_error": float(np.mean(results["tracking_errors"])) ,
        "mean_comm_rate": float(np.mean(results["comm_rates"])) ,
    }

    restore()
    return out


def plot_evaluation(agent, topology, num_tests: int = 3, save_path: str | None = None, max_plot_followers: int | None = 5):
    """ç»˜åˆ¶è¯„ä¼°ç»“æœï¼ˆä½ç½®/é€Ÿåº¦æ›²çº¿ï¼‰ã€‚

    Args:
        agent: æ™ºèƒ½ä½“å®ä¾‹ã€‚
        topology: `CommunicationTopology` å®ä¾‹ã€‚
        num_tests: æµ‹è¯•æ¬¡æ•°ï¼ˆæ¯æ¬¡é‡æ–° reset ç¯å¢ƒå¹¶é‡‡æ ·ä¸€æ¡è½¨è¿¹ï¼‰ã€‚
        save_path: è‹¥æä¾›åˆ™ä¿å­˜å›¾ç‰‡åˆ°è¯¥è·¯å¾„ã€‚
        max_plot_followers: æœ€å¤šå±•ç¤ºå¤šå°‘ä¸ª follower æ›²çº¿ï¼›ä¸º None æˆ– <=0 åˆ™å…¨éƒ¨å±•ç¤ºã€‚

    Returns:
        æ¯æ¬¡æµ‹è¯•çš„è¯¯å·®ç»Ÿè®¡åˆ—è¡¨ï¼ˆfinal/avgï¼‰ã€‚
    """

    env = ModelFreeEnv(topology)

    fig, axes = plt.subplots(num_tests, 2, figsize=(14, 4 * num_tests))
    if num_tests == 1:
        axes = axes.reshape(1, -1)

    results = []

    for test_idx in range(int(num_tests)):
        traj = collect_trajectory(agent, env, MAX_STEPS)

        pos_errors = (traj["follower_pos"] - traj["leader_pos"][:, np.newaxis]) ** 2
        final_error = float(np.mean(pos_errors[-1]))
        avg_error = float(np.mean(pos_errors))

        results.append({"final_error": final_error, "avg_error": avg_error})

        n_followers = int(traj["follower_pos"].shape[1])
        if (max_plot_followers is None) or (int(max_plot_followers) <= 0):
            n_show = n_followers
        else:
            n_show = min(int(max_plot_followers), n_followers)

        ax1 = axes[test_idx, 0]
        ax1.plot(traj["times"], traj["leader_pos"], "r-", linewidth=2.5, label="Leader")
        colors = plt.cm.Blues(np.linspace(0.3, 0.9, n_followers))
        for i in range(n_show):
            ax1.plot(
                traj["times"],
                traj["follower_pos"][:, i],
                color=colors[i],
                alpha=0.8,
                linewidth=1.2,
                label=f"F{i+1}",
            )
        ax1.set_xlabel("Time (s)")
        ax1.set_ylabel("Position")
        ax1.set_title(f"Test {test_idx+1}: Position (Final Err: {final_error:.4f})")
        ax1.legend(loc="upper right", fontsize=8)
        ax1.grid(True, alpha=0.3)

        ax2 = axes[test_idx, 1]
        ax2.plot(traj["times"], traj["leader_vel"], "r-", linewidth=2.5, label="Leader")
        for i in range(n_show):
            ax2.plot(
                traj["times"],
                traj["follower_vel"][:, i],
                color=colors[i],
                alpha=0.8,
                linewidth=1.2,
            )
        ax2.set_xlabel("Time (s)")
        ax2.set_ylabel("Velocity")
        ax2.set_title(f"Test {test_idx+1}: Velocity")
        ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        parent = os.path.dirname(str(save_path))
        if parent:
            os.makedirs(parent, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches="tight")
        print(f"ğŸ“ Figure saved to {save_path}")

    plt.show()

    print("\nğŸ“Š CTDE Evaluation Results:")
    print("-" * 40)
    for i, r in enumerate(results):
        print(f"Test {i+1}: Final Err = {r['final_error']:.4f}, Avg Err = {r['avg_error']:.4f}")

    return results
